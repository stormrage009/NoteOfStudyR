[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R语言学习笔记-IntroR",
    "section": "",
    "text": "基于 Pandoc 自定义 block 是一件很有意思的事情，目前不想让模版过于复杂，仅给出几个最常用的例子。如何自定义可以去看谢益辉的新书 https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html。\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\n\n\n\n\n\n\nTip With Caption\n\n\n\nThis is an example of a callout with a caption.\n\n\n\n\n\n\n\n\nExpand To Learn About Collapse\n\n\n\n\n\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default.\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n如何完成从”看懂别人代码”到”自己写代码”的鸿沟？答案是分解问题。\n\n将难以入手打大问题分解为可以逐步解决的小问题。\n用计算机的思维去思考解决每个小问题的步骤。\n借助类比的简单实例和代码片段，梳理详细算法步骤。\n将详细算法步骤用逐片段的编程语法翻译成代码并调试通过。\n写代码时，随时跟踪关注每一步执行的情况，关注执行的结果（变量、数据值）是否达到预期，这一点非常重要。\n\n以上步骤的关键，就是从问题到代码的中间是有包括分解、编译、调试的过程的。任何编程问题都不可能从问题直接到代码，问题直接到代码是一个可怕的思维定式，在自己编写代码时，一定要强迫自己执行1中所示的步骤。\n\n我们通过一个简单的例子来说明如何执行以上步骤。\n\n\n\n\nROC曲线是二分类机器学习模型的性能评价指标，已知测试集或验证集每个样本的真实类别及其模型预测概率值，就可以计算并绘制ROC曲线。\nROC曲线是不同分类阈值上对比真正率（TPR）与假正率（FPR）的曲线，可以通过下图的步骤进行计算。\n\n\n\n\n\n通过以上定义，我们梳理执行第一步：对问题进行梳理。本例的问题可以分解为如小问题：\n\n让分类阈值以某步长在[0,1]上变化取值；\n对某一个阈值：\n\n计算预测类别\n计算混淆矩阵\n计算真正率和假正率\n\n循环迭代，计算所有阈值的真正率和假正率；\n绘制ROC曲线。\n\n根据以上步骤，我们生成一个小数据集进行演示。\n\nlibrary(tidyverse)\ndf <- tibble(\n  ID = 1:10,\n  真实类别 = c(\"Pos\",\"Pos\",\"Pos\",\"Neg\",\"Pos\",\"Neg\",\"Neg\",\"Neg\",\"Pos\",\"Neg\"),\n  预测概率 = c(0.95,0.86,0.69,0.65,0.59,0.52,0.39,0.28,0.15,0.06)\n)\ndf\n\n# A tibble: 10 × 3\n      ID 真实类别 预测概率\n   <int> <chr>       <dbl>\n 1     1 Pos          0.95\n 2     2 Pos          0.86\n 3     3 Pos          0.69\n 4     4 Neg          0.65\n 5     5 Pos          0.59\n 6     6 Neg          0.52\n 7     7 Neg          0.39\n 8     8 Neg          0.28\n 9     9 Pos          0.15\n10    10 Neg          0.06\n\n\n\n先解决某一个阈值的问题，以0.85为例\n\n\n# 在使用tidyverse时，注意要将“真实类别”转化为因子型数据\nc <- 0.85\ndf1 <- df %>% \n  mutate(\n    预测类别 = ifelse(预测概率 >= c, \"Pos\", \"Neg\"),\n    预测类别 = factor(预测类别, levels = c(\"Pos\", \"Neg\")),\n    真实类别 = factor(真实类别, levels = c(\"Pos\", \"Neg\"))\n  )\ndf1\n\n# A tibble: 10 × 4\n      ID 真实类别 预测概率 预测类别\n   <int> <fct>       <dbl> <fct>   \n 1     1 Pos          0.95 Pos     \n 2     2 Pos          0.86 Pos     \n 3     3 Pos          0.69 Neg     \n 4     4 Neg          0.65 Neg     \n 5     5 Pos          0.59 Neg     \n 6     6 Neg          0.52 Neg     \n 7     7 Neg          0.39 Neg     \n 8     8 Neg          0.28 Neg     \n 9     9 Pos          0.15 Neg     \n10    10 Neg          0.06 Neg     \n\n\n\n针对c=0.85的情况计算混淆矩阵\n\n\n# 即统计本来统计为“Pos”预测为“Pos”的有多少，等等\ncm <- table(df1$预测类别, df1$真实类别)\ncm  \n\n     \n      Pos Neg\n  Pos   2   0\n  Neg   3   5\n\n\n\n计算真正率和假正率。这里我们采用R语言的特长，向量化编程来计算实现。\n\n\ncm[\"Pos\", ]/colSums(cm)\n\nPos Neg \n0.4 0.0 \n\n\n到此，我们完成了本例的核心部分，下一步就是要通过迭代，套用以上算法，计算每个阈值的真正率和假正率。同样采用R语言的特性，泛函式编程进行迭代。\n\n# 将上述计算封装为一个函数\ncal_ROC <- function(df, c){\n  df = df %>% \n  mutate(\n    预测类别 = ifelse(预测概率 >= c, \"Pos\", \"Neg\"),\n    预测类别 = factor(预测类别, levels = c(\"Pos\", \"Neg\")),\n    真实类别 = factor(真实类别, levels = c(\"Pos\", \"Neg\")))\n  cm = table(df$预测类别, df$真实类别)\n  t = cm[\"Pos\",] / colSums(cm)\n  list(TPR = t[[1]], FPR = t[[2]])\n}\n# 测试自定义函数计算结果-结果没问题\ncal_ROC(df, 0.85)\n\n$TPR\n[1] 0.4\n\n$FPR\n[1] 0\n\n\n\n将自定义函数应用到每一个阈值，并一步到位的将结果合并，完成绘图所需的数据计算。\n\n\nc <- seq(1, 0, -0.02)\nrocs <- map_dfr(c, cal_ROC, df = df)\nhead(rocs)\n\n# A tibble: 6 × 2\n    TPR   FPR\n  <dbl> <dbl>\n1   0       0\n2   0       0\n3   0       0\n4   0.2     0\n5   0.2     0\n6   0.2     0\n\n\n\n使用ggplot2包绘图。\n\n\nggplot(rocs, aes(FPR, TPR)) +\n  geom_line(size = 2, color = \"steelblue\") +\n  geom_point(shape = \"diamond\", size = 4, color = \"red\") +\n  theme_bw()\n\n\n\n\n根据以上的步骤，我们可以知道在R中使用tidyverse进行数据科学的工作流程如下图。"
  },
  {
    "objectID": "01-data-structrue.html",
    "href": "01-data-structrue.html",
    "title": "1  数据结构",
    "section": "",
    "text": "[[]]和[]："
  },
  {
    "objectID": "01-data-structrue.html#sec:list",
    "href": "01-data-structrue.html#sec:list",
    "title": "1  数据结构",
    "section": "1.1 列表",
    "text": "1.1 列表\n\npurrr包中提供了一些系列操作列表的函数：\n\n\npluck()：同 “[[” 提取列表中的元素。\nkeep()： 保留满足条件的元素。\ndiscard()： 删除满足条件的元素。\ncompact()： 删除列表中的空元素。\nappend()：在列表末尾增加元素。\nflatten()： 摊平列表（只摊平一层）。\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.6     ✓ purrr   0.3.4\n✓ tibble  3.1.7     ✓ dplyr   1.0.9\n✓ tidyr   1.2.0     ✓ stringr 1.4.0\n✓ readr   2.1.2     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\na <- list(A = 1, B = c(TRUE, FALSE), C = c(\"a\", \"b\", \"c\"))\npluck(a$A)\n\n[1] 1\n\nflatten(a)\n\n$A\n[1] 1\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] FALSE\n\n[[4]]\n[1] \"a\"\n\n[[5]]\n[1] \"b\"\n\n[[6]]\n[1] \"c\""
  },
  {
    "objectID": "01-data-structrue.html#sec:factor",
    "href": "01-data-structrue.html#sec:factor",
    "title": "1  数据结构",
    "section": "1.2 因子",
    "text": "1.2 因子\nR 提供了因子这一数据结构（容器），专门用来存放名义型和有序型的分类变量。因子本质上是一个带有水平（level）属性的整数向量，其中 “水平” 是指事前确定可能取值的有限集合。例如，性别有两个水平：男、女。\nforcats包是处理因子的强大工具。"
  },
  {
    "objectID": "01-data-structrue.html#sec:str",
    "href": "01-data-structrue.html#sec:str",
    "title": "1  数据结构",
    "section": "1.3 字符串",
    "text": "1.3 字符串\n字符串是用双引号或单引号括起来的若干字符1，字符串构成的向量，称为字符向量。数据清洗、可视化等操作都会用到字符串处理。\ntidyverse 系列中的 stringr包 包提供了一系列接口一致的、简单易用的字符串操作函数，足以代替 R 自带字符串函数2。\n两点说明：\n\n查找匹配的各个函数，只是查找第一个匹配，要想查找所有匹配，各个函 数都有后缀 _all 版本；\n各个函数中的参数 pattern 都支持用正则表达式。"
  },
  {
    "objectID": "01-data-structrue.html#sec:date",
    "href": "01-data-structrue.html#sec:date",
    "title": "1  数据结构",
    "section": "1.4 日期时间",
    "text": "1.4 日期时间\n日期时间通常以字符串的形式传入R中，然后转化为以数值形式储存的日期时间变量。\nR的内部日期是以1970年1月1日1至今的天数来储存，内部时间则是以1970年1月1日至今的秒数来储存的。\nlubridata包提供了一系列更方便的函数用来生成、管理日期。\n\n\n\n\n\n\nNote\n\n\n\n不要直接按字符串处理日期时间，应该先转换为日期对象后在处理。\n\n\n\n1.4.1 识别日期时间\n\n识别当前时间\n\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ntoday()\n\n[1] \"2022-05-23\"\n\nnow()\n\n[1] \"2022-05-23 21:55:52 CST\"\n\n\n\n时间日期转换\n\n\n# 将日期型转换为时间型\nas_datetime(today())\n\n[1] \"2022-05-23 UTC\"\n\n# 将时间型转换为日期型\nas_date(now())\n\n[1] \"2022-05-23\"\n\n\n\n\n\n\n\n\nNote\n\n\n\n可根据需要使用 ymd_h/myd_hm/dmy_hms等任意组合识别日期时间值，使用参数tz可以指定时区。\n\nymd(\"20220301\")\n\n[1] \"2022-03-01\"\n\nymd_hms(\"20220301-131300\")\n\n[1] \"2022-03-01 13:13:00 UTC\"\n\n\n\n\n\n创建时间日期\n\n\nmake_date(2020, 8, 27)\n\n[1] \"2020-08-27\"\n\nmake_datetime(2020, 8, 27, 21, 27, 15)\n\n[1] \"2020-08-27 21:27:15 UTC\"\n\n\n\n\n1.4.2 格式化输出日期时间\n\nformat()函数\n\n\nd <- make_date(2022, 4, 22)\nformat(d, \"%Y/%m/%d\")\n\n[1] \"2022/04/22\"\n\n\n\nstamp()函数，按给定模版的格式输出\n\n\nt <- make_datetime(2022, 4, 22, 17, 34, 15)\nfmt <- stamp(\"Created on Sunday, jan 1, 1999 3:34 pm\") # 模版，自动匹配时间组件\n\nMultiple formats matched: \"Created on Sunday, %Om %d, %Y %H:%M %Op\"(1), \"Created on Sunday, %Om %d, %Y %H:%M pm\"(1)\n\n\nUsing: \"Created on Sunday, %Om %d, %Y %H:%M %Op\"\n\nfmt(t)\n\n[1] \"Created on Sunday, 04 22, 2022 17:34 下午\"\n\n\n\n\n1.4.3 提取日期时间数据的组建\n\n\n\n\n\n时间组件、描述、示例\n\n\n\n\n\nt <- ymd_hms(\"2022-04-22 17:41:45\")\n# 提取年份，类似的还有month()、day()、hour()、minute()、second()\nyear(t)\n\n[1] 2022\n\n# 提取季度\nquarter(t)\n\n[1] 2\n\n# 计算时间是当年的第几天\nyday(t)\n\n[1] 112\n\n# 关于星期\nweekdays(t)\n\n[1] \"星期五\"\n\nwday(t)  # 日期是本周的第几天，注意默认周日为第1天\n\n[1] 6\n\nwday(t, label = TRUE)\n\n[1] 周五\nLevels: 周日 < 周一 < 周二 < 周三 < 周四 < 周五 < 周六\n\nweek(t)  # 本周是当年的第几周\n\n[1] 16\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n使用**_tz()函数修改时区：\n\n\ntz()显示当前时区\nwith_tz()将时间数据转换为另一个时区的同一时间\nforce_tz()将时间数据的时区强制转换为另一个时区\n\n\ntz(t)\n\n[1] \"UTC\"\n\nwith_tz(t, tz = \"America/New_York\")\n\n[1] \"2022-04-22 13:41:45 EDT\"\n\nforce_tz(t, tz = \"America/New_York\")\n\n[1] \"2022-04-22 17:41:45 EDT\"\n\n\n\n模糊提取到不同的时间单位，以下函数均需设定取整单位，默认为second\n\n\n使用round_date()四舍五入时间\n使用floor_date()和ceiling_date()向上或向下取整时间\n\n\nround_date(t, unit = \"hour\")  # 按小时四舍五入取整\n\n[1] \"2022-04-22 18:00:00 UTC\"\n\nfloor_date(t, unit = \"hour\")\n\n[1] \"2022-04-22 17:00:00 UTC\"\n\nceiling_date(t, unit = \"hour\")\n\n[1] \"2022-04-22 18:00:00 UTC\"\n\n\n\n\n\n\n1.4.4 时间段数据\n\ninterval()函数计算两个时间点的时间间隔，返回时间段数据。通常需要对时间段进行进一步识别。\n\n\nbegin <- ymd_hm(\"2019-08-10, 14:00\")\nend <- ymd_hm(\"2022-4-26, 9:10\")\ngap <- interval(begin, end)\ngap  # 返回时间段数据\n\n[1] 2019-08-10 14:00:00 UTC--2022-04-26 09:10:00 UTC\n\n# 计算时间段有几天，同理可以计算有几个月、几分钟等\ntime_length(gap, \"day\")\n\n[1] 989.7986\n\n\n\nduration()函数和period()函数计算数值+时间单位存储时段的长度。 period()函数考虑到了闰年和闰秒，所以在国内更推荐使用。\n\n\nperiod(100, units = \"seconds\")  # 100天有多少秒、共几个星期\n\n[1] \"100S\"\n\nperiod(c(90, 5), c(\"second\", \"minute\"))\n\n[1] \"5M 90S\"\n\n\n(关于period()和durarion()两个函数及其附属函数的更多用法)[.todo]\n(日期/时间的计算)[.todo]"
  },
  {
    "objectID": "02-loop-purrr.html",
    "href": "02-loop-purrr.html",
    "title": "3  循环和purrr",
    "section": "",
    "text": "R循环更建议使用purrr的泛函式循环迭代。"
  },
  {
    "objectID": "02-loop-purrr.html#例1-对数据框逐列迭代",
    "href": "02-loop-purrr.html#例1-对数据框逐列迭代",
    "title": "3  循环和purrr",
    "section": "3.1 例1 对数据框逐列迭代",
    "text": "3.1 例1 对数据框逐列迭代\n数据框是序列，第1个元素是第1列df[[1]]，第2个元素是第二列df[[2]]：\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n# 提取\ndf <- iris[, 1:4]\ndf\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width\n1            5.1         3.5          1.4         0.2\n2            4.9         3.0          1.4         0.2\n3            4.7         3.2          1.3         0.2\n4            4.6         3.1          1.5         0.2\n5            5.0         3.6          1.4         0.2\n6            5.4         3.9          1.7         0.4\n7            4.6         3.4          1.4         0.3\n8            5.0         3.4          1.5         0.2\n9            4.4         2.9          1.4         0.2\n10           4.9         3.1          1.5         0.1\n11           5.4         3.7          1.5         0.2\n12           4.8         3.4          1.6         0.2\n13           4.8         3.0          1.4         0.1\n14           4.3         3.0          1.1         0.1\n15           5.8         4.0          1.2         0.2\n16           5.7         4.4          1.5         0.4\n17           5.4         3.9          1.3         0.4\n18           5.1         3.5          1.4         0.3\n19           5.7         3.8          1.7         0.3\n20           5.1         3.8          1.5         0.3\n21           5.4         3.4          1.7         0.2\n22           5.1         3.7          1.5         0.4\n23           4.6         3.6          1.0         0.2\n24           5.1         3.3          1.7         0.5\n25           4.8         3.4          1.9         0.2\n26           5.0         3.0          1.6         0.2\n27           5.0         3.4          1.6         0.4\n28           5.2         3.5          1.5         0.2\n29           5.2         3.4          1.4         0.2\n30           4.7         3.2          1.6         0.2\n31           4.8         3.1          1.6         0.2\n32           5.4         3.4          1.5         0.4\n33           5.2         4.1          1.5         0.1\n34           5.5         4.2          1.4         0.2\n35           4.9         3.1          1.5         0.2\n36           5.0         3.2          1.2         0.2\n37           5.5         3.5          1.3         0.2\n38           4.9         3.6          1.4         0.1\n39           4.4         3.0          1.3         0.2\n40           5.1         3.4          1.5         0.2\n41           5.0         3.5          1.3         0.3\n42           4.5         2.3          1.3         0.3\n43           4.4         3.2          1.3         0.2\n44           5.0         3.5          1.6         0.6\n45           5.1         3.8          1.9         0.4\n46           4.8         3.0          1.4         0.3\n47           5.1         3.8          1.6         0.2\n48           4.6         3.2          1.4         0.2\n49           5.3         3.7          1.5         0.2\n50           5.0         3.3          1.4         0.2\n51           7.0         3.2          4.7         1.4\n52           6.4         3.2          4.5         1.5\n53           6.9         3.1          4.9         1.5\n54           5.5         2.3          4.0         1.3\n55           6.5         2.8          4.6         1.5\n56           5.7         2.8          4.5         1.3\n57           6.3         3.3          4.7         1.6\n58           4.9         2.4          3.3         1.0\n59           6.6         2.9          4.6         1.3\n60           5.2         2.7          3.9         1.4\n61           5.0         2.0          3.5         1.0\n62           5.9         3.0          4.2         1.5\n63           6.0         2.2          4.0         1.0\n64           6.1         2.9          4.7         1.4\n65           5.6         2.9          3.6         1.3\n66           6.7         3.1          4.4         1.4\n67           5.6         3.0          4.5         1.5\n68           5.8         2.7          4.1         1.0\n69           6.2         2.2          4.5         1.5\n70           5.6         2.5          3.9         1.1\n71           5.9         3.2          4.8         1.8\n72           6.1         2.8          4.0         1.3\n73           6.3         2.5          4.9         1.5\n74           6.1         2.8          4.7         1.2\n75           6.4         2.9          4.3         1.3\n76           6.6         3.0          4.4         1.4\n77           6.8         2.8          4.8         1.4\n78           6.7         3.0          5.0         1.7\n79           6.0         2.9          4.5         1.5\n80           5.7         2.6          3.5         1.0\n81           5.5         2.4          3.8         1.1\n82           5.5         2.4          3.7         1.0\n83           5.8         2.7          3.9         1.2\n84           6.0         2.7          5.1         1.6\n85           5.4         3.0          4.5         1.5\n86           6.0         3.4          4.5         1.6\n87           6.7         3.1          4.7         1.5\n88           6.3         2.3          4.4         1.3\n89           5.6         3.0          4.1         1.3\n90           5.5         2.5          4.0         1.3\n91           5.5         2.6          4.4         1.2\n92           6.1         3.0          4.6         1.4\n93           5.8         2.6          4.0         1.2\n94           5.0         2.3          3.3         1.0\n95           5.6         2.7          4.2         1.3\n96           5.7         3.0          4.2         1.2\n97           5.7         2.9          4.2         1.3\n98           6.2         2.9          4.3         1.3\n99           5.1         2.5          3.0         1.1\n100          5.7         2.8          4.1         1.3\n101          6.3         3.3          6.0         2.5\n102          5.8         2.7          5.1         1.9\n103          7.1         3.0          5.9         2.1\n104          6.3         2.9          5.6         1.8\n105          6.5         3.0          5.8         2.2\n106          7.6         3.0          6.6         2.1\n107          4.9         2.5          4.5         1.7\n108          7.3         2.9          6.3         1.8\n109          6.7         2.5          5.8         1.8\n110          7.2         3.6          6.1         2.5\n111          6.5         3.2          5.1         2.0\n112          6.4         2.7          5.3         1.9\n113          6.8         3.0          5.5         2.1\n114          5.7         2.5          5.0         2.0\n115          5.8         2.8          5.1         2.4\n116          6.4         3.2          5.3         2.3\n117          6.5         3.0          5.5         1.8\n118          7.7         3.8          6.7         2.2\n119          7.7         2.6          6.9         2.3\n120          6.0         2.2          5.0         1.5\n121          6.9         3.2          5.7         2.3\n122          5.6         2.8          4.9         2.0\n123          7.7         2.8          6.7         2.0\n124          6.3         2.7          4.9         1.8\n125          6.7         3.3          5.7         2.1\n126          7.2         3.2          6.0         1.8\n127          6.2         2.8          4.8         1.8\n128          6.1         3.0          4.9         1.8\n129          6.4         2.8          5.6         2.1\n130          7.2         3.0          5.8         1.6\n131          7.4         2.8          6.1         1.9\n132          7.9         3.8          6.4         2.0\n133          6.4         2.8          5.6         2.2\n134          6.3         2.8          5.1         1.5\n135          6.1         2.6          5.6         1.4\n136          7.7         3.0          6.1         2.3\n137          6.3         3.4          5.6         2.4\n138          6.4         3.1          5.5         1.8\n139          6.0         3.0          4.8         1.8\n140          6.9         3.1          5.4         2.1\n141          6.7         3.1          5.6         2.4\n142          6.9         3.1          5.1         2.3\n143          5.8         2.7          5.1         1.9\n144          6.8         3.2          5.9         2.3\n145          6.7         3.3          5.7         2.5\n146          6.7         3.0          5.2         2.3\n147          6.3         2.5          5.0         1.9\n148          6.5         3.0          5.2         2.0\n149          6.2         3.4          5.4         2.3\n150          5.9         3.0          5.1         1.8\n\n# 求各列平均值\nmap_dbl(df, mean)  # dbl返回浮点数。\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n# 各列归一化\nrescale <- function(x, type = \"pos\"){\n  rng = range(x, na.rm =  TRUE)  # 提取最小值和最大值\n  if(type == \"pos\"){\n    (x - rng[1]) / (rng[2] - rng[1])\n  } else {\n    (rng[2] - x) / (rng[2] - rng[1])\n  }\n}\nmap_dfc(df, rescale)\n\n# A tibble: 150 × 4\n   Sepal.Length Sepal.Width Petal.Length Petal.Width\n          <dbl>       <dbl>        <dbl>       <dbl>\n 1       0.222        0.625       0.0678      0.0417\n 2       0.167        0.417       0.0678      0.0417\n 3       0.111        0.5         0.0508      0.0417\n 4       0.0833       0.458       0.0847      0.0417\n 5       0.194        0.667       0.0678      0.0417\n 6       0.306        0.792       0.119       0.125 \n 7       0.0833       0.583       0.0678      0.0833\n 8       0.194        0.583       0.0847      0.0417\n 9       0.0278       0.375       0.0678      0.0417\n10       0.167        0.458       0.0847      0     \n# … with 140 more rows"
  },
  {
    "objectID": "02-loop-purrr.html#例2-对数据框逐行迭代",
    "href": "02-loop-purrr.html#例2-对数据框逐行迭代",
    "title": "3  循环和purrr",
    "section": "3.2 例2 对数据框逐行迭代",
    "text": "3.2 例2 对数据框逐行迭代\n\npmap_dbl(df[1:10, ], ~ mean(c(...)))\n\n [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400\n\nmap_dbl(asplit(df[1:10, ], 1), mean)\n\n    1     2     3     4     5     6     7     8     9    10 \n2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400"
  },
  {
    "objectID": "02-loop-purrr.html#例3-批量读取数据并按行合并",
    "href": "02-loop-purrr.html#例3-批量读取数据并按行合并",
    "title": "3  循环和purrr",
    "section": "3.3 例3 批量读取数据并按行合并",
    "text": "3.3 例3 批量读取数据并按行合并\n\nlibrary(readxl)\n# 来自多个文件\nfiles <- list.files(\"datas/read_datas\", pattern = \"xlsx\", full.names = TRUE)\ndf2 <- map_dfr(set_names(files), read_excel, .id = \"from\")\ndf2\n\n# A tibble: 16 × 7\n   from                                班级  姓名   性别   语文  数学  英语\n   <chr>                               <chr> <chr>  <chr> <dbl> <dbl> <dbl>\n 1 datas/read_datas/六1班学生成绩.xlsx 六1班 何娜   女       87    92    79\n 2 datas/read_datas/六1班学生成绩.xlsx 六1班 黄才菊 女       95    77    75\n 3 datas/read_datas/六1班学生成绩.xlsx 六1班 陈芳妹 女       79    87    66\n 4 datas/read_datas/六1班学生成绩.xlsx 六1班 陈学勤 男       82    79    66\n 5 datas/read_datas/六3班学生成绩.xlsx 六3班 江佳欣 女       80    69    75\n 6 datas/read_datas/六3班学生成绩.xlsx 六3班 何诗婷 女       76    53    72\n 7 datas/read_datas/六3班学生成绩.xlsx 六3班 林可莉 女       72    52    72\n 8 datas/read_datas/六3班学生成绩.xlsx 六3班 雷帆   男       78    56    66\n 9 datas/read_datas/六4班学生成绩.xlsx 六4班 周婵   女       92    94    77\n10 datas/read_datas/六4班学生成绩.xlsx 六4班 李小龄 男       90    87    69\n11 datas/read_datas/六4班学生成绩.xlsx 六4班 陈丽丽 女       87    93    61\n12 datas/read_datas/六4班学生成绩.xlsx 六4班 杨昌晟 男       84    85    64\n13 datas/read_datas/六5班学生成绩.xlsx 六5班 符苡榕 女       85    89    76\n14 datas/read_datas/六5班学生成绩.xlsx 六5班 陆曼   女       88    84    69\n15 datas/read_datas/六5班学生成绩.xlsx 六5班 容唐   女       83    71    56\n16 datas/read_datas/六5班学生成绩.xlsx 六5班 蒙丽梅 女       72    72    64\n\n# 来自同一个文件的多个sheet\npath <- \"datas/学生成绩.xlsx\"\ndf3 <- map_dfr(excel_sheets(path), ~ read_excel(path, sheet = .x), .id = \"from\")\ndf3\n\n# A tibble: 20 × 7\n   from  班级  姓名   性别   语文  数学  英语\n   <chr> <chr> <chr>  <chr> <dbl> <dbl> <dbl>\n 1 1     六1班 何娜   女       87    92    79\n 2 1     六1班 黄才菊 女       95    77    75\n 3 1     六1班 陈芳妹 女       79    87    66\n 4 1     六1班 陈学勤 男       82    79    66\n 5 2     六2班 黄祖娜 女       94    88    75\n 6 2     六2班 徐雅琦 女       92    86    72\n 7 2     六2班 徐达政 男       90    86    72\n 8 2     六2班 陈华健 男       92    84    70\n 9 3     六3班 江佳欣 女       80    69    75\n10 3     六3班 何诗婷 女       76    53    72\n11 3     六3班 林可莉 女       72    52    72\n12 3     六3班 雷帆   男       78    56    66\n13 4     六4班 周婵   女       92    94    77\n14 4     六4班 李小龄 男       90    87    69\n15 4     六4班 陈丽丽 女       87    93    61\n16 4     六4班 杨昌晟 男       84    85    64\n17 5     六5班 符苡榕 女       85    89    76\n18 5     六5班 陆曼   女       88    84    69\n19 5     六5班 容唐   女       83    71    56\n20 5     六5班 蒙丽梅 女       72    72    64"
  },
  {
    "objectID": "03-function.html",
    "href": "03-function.html",
    "title": "4  自定义函数",
    "section": "",
    "text": "通过一个例子来学习如何自定义一个函数"
  },
  {
    "objectID": "03-function.html#自定义函数把百分制分数转化为五分制分数的功能",
    "href": "03-function.html#自定义函数把百分制分数转化为五分制分数的功能",
    "title": "4  自定义函数",
    "section": "4.1 自定义函数把百分制分数转化为五分制分数的功能",
    "text": "4.1 自定义函数把百分制分数转化为五分制分数的功能\n第一步，分析输入和输出，设计函数外形\n\n输入有几个，分别是什么，适合用什么数据类型存放？\n输出有几个，分别是什么，适合用什么数据类型存放？\n\n这个问题的输入有1个：百分制分数，数值型；输出有1个：五分制分数，字符串。在此基础设计自定义函数的外形。\n\nScore_Conv <- function(score) {\n  # 实现将一个百分制分数转化为五级分数\n  # 输入参数：score为数值型，百分制分数\n  # 返回值：res为字符串型，五分制分数\n  ...\n}\n\n第二步，梳理功能实现的过程\n\n分解问题+实例梳理+翻译及调试\n拿一组本例的具体形参的值作为输入，如如何将76分转换为五分制分数”良”，这依赖于对五级分数界限的选取，选定后可以使用分支判断实现。逐步调试，得到正确的返回值结果，这步很关键，是实现下一步复杂功能的基础。\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nscore <- 76\nif (score >= 90) {\n  res <- \"优\"\n} else if (score >= 80) {\n  res <- \"良\"\n} else if (score >= 70) {\n  res <- \"中\"\n} else if (score >= 60) {\n  res <- \"及格\"\n} else {\n  res <- \"不及格\"\n}\nres\n\n[1] \"中\"\n\n\n第三步，将第二部的代码封装成为函数体\n\nScore_Conv <- function(score) {\n  if (score >= 90) {\n    res <- \"优\"\n  } else if (score >= 80) {\n    res <- \"良\"\n  } else if (score >= 70) {\n    res <- \"中\"\n  } else if (score >= 60) {\n    res <- \"及格\"\n  } else {\n    res <- \"不及格\"\n  }\n  res\n}\n\n在函数编制好后，我们可以尝试调用它\n\nScore_Conv(76)\n\n[1] \"中\""
  },
  {
    "objectID": "03-function.html#改进函数-函数向量化",
    "href": "03-function.html#改进函数-函数向量化",
    "title": "4  自定义函数",
    "section": "4.2 改进函数-函数向量化",
    "text": "4.2 改进函数-函数向量化\n目前的函数仅能输入一个参数，在实际下通常需要输入多个参数。我们有两种方法实现\n方法1：直接修改自定义函数\n输入的参数为一个数值向量，对函数体进行修改。\n\nScore_Conv2 <- function(score) {\n  n <- length(score)\n  res <- vector(\"character\", n)\n  for (i in 1:n) {\n    if (score[i] >= 90) {\n      res[i] <- \"优\"\n    } else if (score[i] >= 80) {\n      res[i] <- \"良\"\n    } else if (score[i] >= 70) {\n      res[i] <- \"中\"\n    } else if (score[i] >= 60) {\n      res[i] <- \"及格\"\n    } else {\n      res[i] <- \"不及格\"\n    }\n  }\n  res\n}\n\n# 测试函数\nScore_Conv2(c(35, 67, 100))\n\n[1] \"不及格\" \"及格\"   \"优\"    \n\n\n方法2：直接使用map系列函数\n得益于purrr中的map系列函数，我们可以将一个函数批量用在一些列元素中，从而达到不修改原函数而实现向量化操作。\n\nscores <- c(35, 67, 100)\nmap_chr(scores, Score_Conv)\n\n[1] \"不及格\" \"及格\"   \"优\""
  },
  {
    "objectID": "03-function.html#返回多个处理值",
    "href": "03-function.html#返回多个处理值",
    "title": "4  自定义函数",
    "section": "4.3 返回多个处理值",
    "text": "4.3 返回多个处理值\n如果需要返回多个处理值，则需要将多个值大包成一个列表（或数据框）再返回。\n\n# 自定义函数，实现计算一个数值型向量的均值和标准差。\nMeanSad <- function(x) {\n  mu <- mean(x)\n  std <- sqrt(sum((x - mu)^2) / (length(x) - 1))\n  paste(paste(\"均值为\", mu),\" \", paste(\"标准差为\", std))\n}\n\nMeanSad(c(2, 4, 6, 9, 12))\n\n[1] \"均值为 6.6   标准差为 3.97492138287036\""
  },
  {
    "objectID": "03-function.html#参数",
    "href": "03-function.html#参数",
    "title": "4  自定义函数",
    "section": "4.4 ...参数",
    "text": "4.4 ...参数\n一般函数参数只接受一个对象，比如对两个数加和的函数，给它 3 个数加和就会报错\n...参数可以接受多个对象，并将其打包为一个列表传递给函数体\n\nmy_sum2 <- function(...){\n  sum(...)\n}\nmy_sum2(1, 2, 3, 4, 5, 6)\n\n[1] 21\n\n\n\n\n\n\n\n\nDanger\n\n\n\nr中常用的概率函数中，不同前缀的含义分别如下：\n\nd：密度函数（density）\np：分布函数（distribution）\nq：分位数函数（quantile）\nr：随机数函数（random）\n\n上述四个字母+分布缩写，就构成通常使用的概率函数\n\ndnorm(3, 0, 2)  # 正态分布N(0, 4)在3处的密度值\n\n[1] 0.0647588\n\n\n\nrnorm(5, 0, 1)  # 生成5个服从N(0, 1)分布的随机数\n\n[1] -0.3192688  1.3478645  0.4747361  1.1571174 -0.4027171\n\n\n\n\n\n\n\n\n\n\n\n\n(时间序列函数)[.todo]"
  },
  {
    "objectID": "04-data-manipulation.html#筛选分组汇总",
    "href": "04-data-manipulation.html#筛选分组汇总",
    "title": "5  数据操作",
    "section": "5.2 筛选、分组、汇总",
    "text": "5.2 筛选、分组、汇总"
  },
  {
    "objectID": "04-data-manipulation.html#其它操作",
    "href": "04-data-manipulation.html#其它操作",
    "title": "5  数据操作",
    "section": "5.3 其它操作",
    "text": "5.3 其它操作"
  },
  {
    "objectID": "04-data-manipulation.html#神器data.table",
    "href": "04-data-manipulation.html#神器data.table",
    "title": "5  数据操作",
    "section": "5.4 神器data.table",
    "text": "5.4 神器data.table"
  },
  {
    "objectID": "06-Statistical-modeling.html",
    "href": "06-Statistical-modeling.html",
    "title": "7  统计建模",
    "section": "",
    "text": "使用broom包实现将模型输出结果转化为整洁且列名规范一致的tibble，方面后续工作取用\n再与tidyr::nest()/unest()以及purrr::map()连用，实现批量建模和整合模型结果的效果"
  },
  {
    "objectID": "06-Statistical-modeling.html#broom包整合模型结果",
    "href": "06-Statistical-modeling.html#broom包整合模型结果",
    "title": "7  统计建模",
    "section": "7.1 broom包整合模型结果",
    "text": "7.1 broom包整合模型结果\n\ntidy()：模型系数估计及其统计量\n\n\n返回结果tibble的每一行均具有明确含义的项目\n各列包括：\n\nterm：回归或模型中要估计的项\nestimate：参数估计值\nstatistic：检验统计量\np.value：检验统计量的p值\nconf.low，conf.high：estimate的置信区间\ndf：自由度\n\n\n\nglance()：模型诊断信息\n\n返回一行的tilbble，各列是模型针对信息：\n\nr.squared: R2R^2\nadj.r.squared：根据自由度修正的R2R^2\nsigma：残差标准差估计值\nAIC，BIC：信息准则\n\n\naugment()：增加预测值列、残差列等\n\n\naugment(model, data, newdata)：如果data参数丢失，则不包含原始数据；如果设置newdata，则只针对新数据。\n返回结果tibble的每一行都对应原始数据或新数据的一行\n新增加的列包括：\n\n.fitted：预测值，与原始数据同量纲\n.resid：残差，真实值与预测值的差\n.cluster：聚类结果\n\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(broom)\nmodel <- lm(mpg ~ wt, data = mtcars)\nmodel %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    37.3      1.88      19.9  8.24e-19\n2 wt             -5.34     0.559     -9.56 1.29e-10\n\nmodel %>% \n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.753         0.745  3.05      91.4 1.29e-10     1  -80.0  166.  170.\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nmodel %>% \n  augment()\n\n# A tibble: 32 × 9\n   .rownames           mpg    wt .fitted .resid   .hat .sigma .cooksd .std.resid\n   <chr>             <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n 1 Mazda RX4          21    2.62    23.3 -2.28  0.0433   3.07 1.33e-2    -0.766 \n 2 Mazda RX4 Wag      21    2.88    21.9 -0.920 0.0352   3.09 1.72e-3    -0.307 \n 3 Datsun 710         22.8  2.32    24.9 -2.09  0.0584   3.07 1.54e-2    -0.706 \n 4 Hornet 4 Drive     21.4  3.22    20.1  1.30  0.0313   3.09 3.02e-3     0.433 \n 5 Hornet Sportabout  18.7  3.44    18.9 -0.200 0.0329   3.10 7.60e-5    -0.0668\n 6 Valiant            18.1  3.46    18.8 -0.693 0.0332   3.10 9.21e-4    -0.231 \n 7 Duster 360         14.3  3.57    18.2 -3.91  0.0354   3.01 3.13e-2    -1.31  \n 8 Merc 240D          24.4  3.19    20.2  4.16  0.0313   3.00 3.11e-2     1.39  \n 9 Merc 230           22.8  3.15    20.5  2.35  0.0314   3.07 9.96e-3     0.784 \n10 Merc 280           19.2  3.44    18.9  0.300 0.0329   3.10 1.71e-4     0.100 \n# … with 22 more rows\n\n\n有了以上信息，我们就可以方便的进行数据筛选或者绘图\n\nmodel %>% \n  augment() %>% \n  ggplot(aes(x = wt, y = mpg)) +\n    geom_point() +\n    geom_line(aes(y = .fitted), color = \"blue\") +\n    geom_segment(aes(xend = wt, yend = .fitted),\n                 color = \"red\")\n\n\n\n\n\nmodel %>% \n   augment() %>% \n   ggplot(aes(wt, .resid)) +\n      geom_point() +\n      geom_hline(yintercept = 0, color = \"blue\")"
  },
  {
    "objectID": "06-Statistical-modeling.html#modelr包辅助建模",
    "href": "06-Statistical-modeling.html#modelr包辅助建模",
    "title": "7  统计建模",
    "section": "7.2 modelr包辅助建模",
    "text": "7.2 modelr包辅助建模\nmodelr包提供一系列辅助建模的函数，便于在tidyverse框架下辅助建模教学。\n\n7.2.1 重抽样1 resample_*()：\n\n\n\n\n\n\nDanger\n\n\n\n常用的重抽样方法有：简单重抽样（Holdout），自助重抽样（Bootstrap），交叉验证重抽样（Cross Validation），置换重抽样（Permutation）\n\n\nresample_*()系列函数的具体用法和参数如下：\n\nresample(data, idx)：根据整数向量idx从数据集data中重抽样。\nresample_partition(data, p)：生成一个简单重抽样，按概率p对数据集进行划分。常用于划分训练集和测试集。\nresample_bootstrap(data)：生成一个自助重抽样。类似的，bootstrap(data, n)则生成n个自助重抽样。\ncross_kfold(data, k)：生成k折交叉验证重抽样。类似的：\n\ncross_loo(data)：生成留一交叉验证重抽样\ncross_mc(data, n, test)：按测试集占比test，生成n对蒙特卡洛交叉验证\n\nresample_permutaiton(data, columns)：按列生成1个置换重抽样，类似的，permutation(data, n, columns)则生成n个置换重抽样。\n\n\n\n\n\n\n\nDanger\n\n\n\n\n为避免低效操作数据，重抽样的结果都是保存原数据的指针。\n重抽样数据集均存放在返回结果的列表列，借助map()系列函数可以方便的进行批量建模。\n每个重抽样的数据集，利用as.data.frame()或as_tibble()函数可以转化为数据框，也可以不用转换而直接应用于模型函数\n\n\n\n\n\n7.2.2 模型性能度量函数\n我们通常使用一些统计量对所构建模型的精度进行度量，常用的有：\n\nrmse(model, data)：均方根误差\nmse(model, data)：均方误差\nmae(model, data)：平均绝对误差\nqae(model, data, probs)：分位数绝对误差\nmape(model, data)：平均绝对百分比误差\nrsae(model, data)：绝对误差相对和\nrsquare(model, data)：R2R^2\n\n\n\n7.2.3 生成模型数据\n\nseq_range(x, n)：根据向量x值范围生成等间隔序列。\ndata_grid(data, f1, f2)： 生成唯一值的所有组合。\nmodel_matrix()： 生成模型（设计矩阵），特别是用于虚拟变量梳理\n\n\n\n7.2.4 增加预测值列、残差列\n\nadd_predictions()\nadd_residuals()\n\n\nlibrary(modelr)\n\n\n载入程辑包：'modelr'\n\n\nThe following object is masked from 'package:broom':\n\n    bootstrap\n\nex <- resample_partition(mtcars, c(test = 0.3, train = 0.7))\nmodel <- lm(mpg ~ wt, data = ex$train)  # 建立线性模型\nrmse(model, ex$test)  # 利用测试集计算构建模型的均方根误差\n\n[1] 3.773909\n\nmodel <- lm(mpg ~ wt + cyl + vs, data = mtcars)\ndata_grid(mtcars, wt = seq_range(wt, 10), cyl, vs) %>% \n   add_predictions(model)\n\n# A tibble: 60 × 4\n      wt   cyl    vs  pred\n   <dbl> <dbl> <dbl> <dbl>\n 1  1.51     4     0  28.4\n 2  1.51     4     1  28.9\n 3  1.51     6     0  25.6\n 4  1.51     6     1  26.2\n 5  1.51     8     0  22.9\n 6  1.51     8     1  23.4\n 7  1.95     4     0  27.0\n 8  1.95     4     1  27.5\n 9  1.95     6     0  24.2\n10  1.95     6     1  24.8\n# … with 50 more rows"
  },
  {
    "objectID": "06-Statistical-modeling.html#案例110折交叉验证",
    "href": "06-Statistical-modeling.html#案例110折交叉验证",
    "title": "7  统计建模",
    "section": "7.3 案例1，10折交叉验证",
    "text": "7.3 案例1，10折交叉验证\n\n\n\n\n\n\nDanger\n\n\n\n通常将数据集划分为训练集（90%）和测试集（10%），在训练集上训练一个模型，在测试机上评估模型效果。\n如果只这样做一轮的话，模型的效果可能具有偶然性，对数据集利用的也不够充分。k折交叉验证是克服该缺陷的更好做法。\n\n\n10折交叉验证，就是将数据集随机分成10份，分别以其中1份为测试集，其余9份为训练集，组成10组数据，训练10个模型，评估10次模型效果，取平均作为最终模型效果。\n\nlibrary(tidyverse)\n# 生成10折交叉验证的数据\ncv10 <- crossv_kfold(mtcars, 10)\ncv10  # 结果为10行的嵌套数据框，分对对应10组训练集和测试集\n\n# A tibble: 10 × 3\n   train                test                .id  \n   <named list>         <named list>        <chr>\n 1 <resample [28 x 11]> <resample [4 x 11]> 01   \n 2 <resample [28 x 11]> <resample [4 x 11]> 02   \n 3 <resample [29 x 11]> <resample [3 x 11]> 03   \n 4 <resample [29 x 11]> <resample [3 x 11]> 04   \n 5 <resample [29 x 11]> <resample [3 x 11]> 05   \n 6 <resample [29 x 11]> <resample [3 x 11]> 06   \n 7 <resample [29 x 11]> <resample [3 x 11]> 07   \n 8 <resample [29 x 11]> <resample [3 x 11]> 08   \n 9 <resample [29 x 11]> <resample [3 x 11]> 09   \n10 <resample [29 x 11]> <resample [3 x 11]> 10   \n\n# 批量建模，使用map计算新列+赋值\ncv10 %>% \n   mutate(models = map(train, ~ lm(mpg ~ wt, data = .)),\n          rmse = map2_dbl(models, test, rmse))\n\n# A tibble: 10 × 5\n   train                test                .id   models        rmse\n   <named list>         <named list>        <chr> <named list> <dbl>\n 1 <resample [28 x 11]> <resample [4 x 11]> 01    <lm>          1.31\n 2 <resample [28 x 11]> <resample [4 x 11]> 02    <lm>          3.84\n 3 <resample [29 x 11]> <resample [3 x 11]> 03    <lm>          3.11\n 4 <resample [29 x 11]> <resample [3 x 11]> 04    <lm>          3.08\n 5 <resample [29 x 11]> <resample [3 x 11]> 05    <lm>          2.18\n 6 <resample [29 x 11]> <resample [3 x 11]> 06    <lm>          2.88\n 7 <resample [29 x 11]> <resample [3 x 11]> 07    <lm>          4.38\n 8 <resample [29 x 11]> <resample [3 x 11]> 08    <lm>          3.82\n 9 <resample [29 x 11]> <resample [3 x 11]> 09    <lm>          1.84\n10 <resample [29 x 11]> <resample [3 x 11]> 10    <lm>          4.48"
  },
  {
    "objectID": "06-Statistical-modeling.html#批量建模",
    "href": "06-Statistical-modeling.html#批量建模",
    "title": "7  统计建模",
    "section": "7.4 批量建模",
    "text": "7.4 批量建模\ntidyverse中两种简介的批量建模方式，两种方式均很巧妙。\n\n使用嵌套数据框+purrr::map实现\n使用dplyr包的rowwise技术。\n\n我们以ecostats2数据集为例，分别来看\n\nload(\"datas/ecostats.rda\")\neconomics\n\n# A tibble: 574 × 6\n   date         pce    pop psavert uempmed unemploy\n   <date>     <dbl>  <dbl>   <dbl>   <dbl>    <dbl>\n 1 1967-07-01  507. 198712    12.6     4.5     2944\n 2 1967-08-01  510. 198911    12.6     4.7     2945\n 3 1967-09-01  516. 199113    11.9     4.6     2958\n 4 1967-10-01  512. 199311    12.9     4.9     3143\n 5 1967-11-01  517. 199498    12.8     4.7     3066\n 6 1967-12-01  525. 199657    11.8     4.8     3018\n 7 1968-01-01  531. 199808    11.7     5.1     2878\n 8 1968-02-01  534. 199920    12.3     4.5     3001\n 9 1968-03-01  544. 200056    11.7     4.1     2877\n10 1968-04-01  544  200208    12.3     4.6     2709\n# … with 564 more rows\n\n\n\n7.4.1 使用嵌套数据框+purrr::map方式\n\n想要对每个省份（数据子集）做重复操作\n\n\n首先对数据框使用group_nest()对分组变量Region做分组嵌套，得到嵌套数据框，每组数据作为数据框嵌套到列表data\n嵌套数据框的每一行是一个分组，表示一个省份的整个事件跨度内的所有观测（而不是某个单独时间点的观测）\n\n\n\n\n\n\n\nby_region <- ecostats %>% \n   group_nest(Region)\nby_region$data[[1]]  # 查看列表的第1个元素的内容\n\n# A tibble: 17 × 6\n    Year Electricity Investment Consumption Population gdpPercap\n   <int>       <dbl>      <dbl>       <dbl>      <dbl>     <dbl>\n 1  2001        360.       893.        2739       6128     5716.\n 2  2002        390.      1074.        2988       6144     6230.\n 3  2003        445.      1419.        3312       6163     6990.\n 4  2004        516.      1935.        3707       6228     8236.\n 5  2005        582.      2525.        3870       6120     9274.\n 6  2006        662.      3534.        4409       6110    10639.\n 7  2007        769.      5088.        5276       6118    12981.\n 8  2008        859.      6747.        6006       6135    15514.\n 9  2009        952.      8991.        6829       6131    17721.\n10  2010       1078.     11543.        8237       5957    22242.\n11  2011       1221.     12456.       10055       5972    27269.\n12  2012       1361.     15426.       10978       5978    30682.\n13  2013       1528.     18622.       11618       5988    34375.\n14  2014       1585.     21876.       12944       5997    37552.\n15  2015       1640.     24386.       13941       6011    39646.\n16  2016       1795.     27033.       15466       6033    43606.\n17  2017       1921.     29275.       17141       6057    48995.\n\n\n\n\n\n\n\n\nDanger\n\n\n\n\n嵌套数据可以使用unnest(by_region, data)解除嵌套。\n嵌套数据框的操作与普通数据框一直，如filter()、mutate()等函数均可以使用。\n\n\n\n\n批量建模\n\n\n对嵌套数据框的data列进行建模，使用mutate()函数增加一列模型列用来存放每个省份对应的data拟合人均消费水平对人均GDP的线性回归模型。\n\n\nby_region <- by_region %>% \n   mutate(model = map(data, ~ lm(Consumption ~ gdpPercap, .x)))\nby_region\n\n# A tibble: 31 × 3\n   Region               data model \n   <chr>  <list<tibble[,6]>> <list>\n 1 安徽             [17 × 6] <lm>  \n 2 北京             [17 × 6] <lm>  \n 3 福建             [17 × 6] <lm>  \n 4 甘肃             [17 × 6] <lm>  \n 5 广东             [17 × 6] <lm>  \n 6 广西             [17 × 6] <lm>  \n 7 贵州             [17 × 6] <lm>  \n 8 海南             [17 × 6] <lm>  \n 9 河北             [17 × 6] <lm>  \n10 河南             [17 × 6] <lm>  \n# … with 21 more rows\n\n\n\n继续修改列，在表中加入模型的判断统计量\n\n\nby_region %>% \n   mutate(rmse = map2_dbl(model, data, rmse),\n          rsq = map2_dbl(model, data, rsquare),\n          slop = map_dbl(model, ~ coef(.x)[[2]]),\n          pval = map_dbl(model, ~ glance(.x)$p.value))\n\n# A tibble: 31 × 7\n   Region               data model   rmse   rsq  slop     pval\n   <chr>  <list<tibble[,6]>> <list> <dbl> <dbl> <dbl>    <dbl>\n 1 安徽             [17 × 6] <lm>    185. 0.998 0.327 2.36e-22\n 2 北京             [17 × 6] <lm>   2005. 0.975 0.392 1.71e-13\n 3 福建             [17 × 6] <lm>    415. 0.996 0.287 2.20e-19\n 4 甘肃             [17 × 6] <lm>    600. 0.976 0.448 1.55e-13\n 5 广东             [17 × 6] <lm>    592. 0.994 0.417 2.40e-18\n 6 广西             [17 × 6] <lm>    270. 0.996 0.433 9.88e-20\n 7 贵州             [17 × 6] <lm>    268. 0.996 0.424 1.17e-19\n 8 海南             [17 × 6] <lm>   1173. 0.955 0.417 1.77e-11\n 9 河北             [17 × 6] <lm>    497. 0.985 0.368 3.32e-15\n10 河南             [17 × 6] <lm>    663. 0.981 0.375 2.33e-14\n# … with 21 more rows\n\n\n\n也可以配合 broom 包的函数 tidy(), glance(), augment() 批量、整洁地提取模型结果，这些结果仍是嵌套的列表列，若要完整地显示出来，需 要借助 unnest() 解除嵌套.\n\n提取模型系数估计及其统计量。\n\n\n\nby_region %>% \n   mutate(result = map(model, tidy)) %>% \n   select(Region, result) %>% \n   unnest(cols = result)\n\n# A tibble: 62 × 6\n   Region term         estimate  std.error statistic  p.value\n   <chr>  <chr>           <dbl>      <dbl>     <dbl>    <dbl>\n 1 安徽   (Intercept)   942.      89.4        10.5   2.47e- 8\n 2 安徽   gdpPercap       0.327    0.00340    96.2   2.36e-22\n 3 北京   (Intercept) -3824.    1301.         -2.94  1.01e- 2\n 4 北京   gdpPercap       0.392    0.0160     24.4   1.71e-13\n 5 福建   (Intercept)  1502.     214.          7.01  4.21e- 6\n 6 福建   gdpPercap       0.287    0.00471    60.9   2.20e-19\n 7 甘肃   (Intercept)  -168.     319.         -0.528 6.05e- 1\n 8 甘肃   gdpPercap       0.448    0.0182     24.6   1.55e-13\n 9 广东   (Intercept)  -487.     363.         -1.34  1.99e- 1\n10 广东   gdpPercap       0.417    0.00804    51.9   2.40e-18\n# … with 52 more rows\n\n\n\n提取模型诊断信息\n\n\nby_region %>% \n   mutate(result = map(model, glance)) %>% \n   select(Region, result) %>% \n   unnest(cols = result)\n\n# A tibble: 31 × 13\n   Region r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n   <chr>      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\n 1 安徽       0.998         0.998  197.     9260. 2.36e-22     1  -113.  232.\n 2 北京       0.975         0.974 2134.      597. 1.71e-13     1  -153.  313.\n 3 福建       0.996         0.996  441.     3713. 2.20e-19     1  -127.  259.\n 4 甘肃       0.976         0.974  638.      605. 1.55e-13     1  -133.  272.\n 5 广东       0.994         0.994  630.     2696. 2.40e-18     1  -133.  271.\n 6 广西       0.996         0.996  287.     4133. 9.88e-20     1  -119.  245.\n 7 贵州       0.996         0.996  286.     4038. 1.17e-19     1  -119.  244.\n 8 海南       0.955         0.952 1249.      315. 1.77e-11     1  -144.  295.\n 9 河北       0.985         0.985  529.     1019. 3.32e-15     1  -130.  265.\n10 河南       0.981         0.980  706.      783. 2.33e-14     1  -135.  275.\n# … with 21 more rows, and 4 more variables: BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>\n\n\n\n批量增加预测值、残差列等\n\n\nby_region %>% \n   mutate(result = map(model, augment)) %>% \n   select(Region, result) %>% \n   unnest(cols = result)\n\n# A tibble: 527 × 9\n   Region Consumption gdpPercap .fitted  .resid   .hat .sigma .cooksd .std.resid\n   <chr>        <dbl>     <dbl>   <dbl>   <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n 1 安徽          2739     5716.   2811.  -72.5  0.140    203. 1.28e-2    -0.396 \n 2 安徽          2988     6230.   2980.    8.49 0.135    204. 1.67e-4     0.0463\n 3 安徽          3312     6990.   3228.   84.0  0.128    203. 1.53e-2     0.456 \n 4 安徽          3707     8236.   3635.   71.7  0.117    203. 9.91e-3     0.387 \n 5 安徽          3870     9274.   3975. -105.   0.109    202. 1.94e-2    -0.564 \n 6 安徽          4409    10639.   4421.  -12.2  0.0986   204. 2.32e-4    -0.0651\n 7 安徽          5276    12981.   5187.   89.0  0.0842   203. 1.02e-2     0.472 \n 8 安徽          6006    15514.   6015.   -9.30 0.0722   204. 9.32e-5    -0.0490\n 9 安徽          6829    17721.   6737.   92.0  0.0648   202. 8.07e-3     0.482 \n10 安徽          8237    22242.   8216.   21.5  0.0588   204. 3.93e-4     0.112 \n# … with 517 more rows"
  },
  {
    "objectID": "06-Statistical-modeling.html#利用rowwise技术",
    "href": "06-Statistical-modeling.html#利用rowwise技术",
    "title": "7  统计建模",
    "section": "7.5 利用rowwise技术",
    "text": "7.5 利用rowwise技术\nrowwise按行方式操作数据，可以理解为一种特殊的分组：每一行为一组\n\nby_region <- ecostats %>% \n   nest_by(Region)\nby_region  # 增加了按行惭怍的分组信息：Rowwise:Region\n\n# A tibble: 31 × 2\n# Rowwise:  Region\n   Region               data\n   <chr>  <list<tibble[,6]>>\n 1 安徽             [17 × 6]\n 2 北京             [17 × 6]\n 3 福建             [17 × 6]\n 4 甘肃             [17 × 6]\n 5 广东             [17 × 6]\n 6 广西             [17 × 6]\n 7 贵州             [17 × 6]\n 8 海南             [17 × 6]\n 9 河北             [17 × 6]\n10 河南             [17 × 6]\n# … with 21 more rows\n\n\nnest_by()将一个省份的数据放在同一行中嵌套，正好契合rowwise的逻辑，即逐行的对每个嵌套的数据框进行建模和提取模型信息的操作，这些操作使用mutate()和summarise()连用来实现，前者会保持rowwise模式（计算结果行数保持不变），而后者相当于对每行结果做汇总，结果行数可变（变多，且结果不具有rowwise模式）。\n\nby_region <- by_region %>% \n   mutate(model = list(lm(Consumption ~ gdpPercap, data)))\nby_region\n\n# A tibble: 31 × 3\n# Rowwise:  Region\n   Region               data model \n   <chr>  <list<tibble[,6]>> <list>\n 1 安徽             [17 × 6] <lm>  \n 2 北京             [17 × 6] <lm>  \n 3 福建             [17 × 6] <lm>  \n 4 甘肃             [17 × 6] <lm>  \n 5 广东             [17 × 6] <lm>  \n 6 广西             [17 × 6] <lm>  \n 7 贵州             [17 × 6] <lm>  \n 8 海南             [17 × 6] <lm>  \n 9 河北             [17 × 6] <lm>  \n10 河南             [17 × 6] <lm>  \n# … with 21 more rows\n\n\n批量建模后，使用模型列和数据列对模型的检验统计量进行计算:\n\n使用mutate:\n\n\nby_region %>% \n   mutate(\n      rmse = rmse(model, data),\n      rsq = rsquare(model, data),\n      slope = coef(model)[[2]],\n      pval = glance(model)$p.value\n   )\n\n# A tibble: 31 × 7\n# Rowwise:  Region\n   Region               data model   rmse   rsq slope     pval\n   <chr>  <list<tibble[,6]>> <list> <dbl> <dbl> <dbl>    <dbl>\n 1 安徽             [17 × 6] <lm>    185. 0.998 0.327 2.36e-22\n 2 北京             [17 × 6] <lm>   2005. 0.975 0.392 1.71e-13\n 3 福建             [17 × 6] <lm>    415. 0.996 0.287 2.20e-19\n 4 甘肃             [17 × 6] <lm>    600. 0.976 0.448 1.55e-13\n 5 广东             [17 × 6] <lm>    592. 0.994 0.417 2.40e-18\n 6 广西             [17 × 6] <lm>    270. 0.996 0.433 9.88e-20\n 7 贵州             [17 × 6] <lm>    268. 0.996 0.424 1.17e-19\n 8 海南             [17 × 6] <lm>   1173. 0.955 0.417 1.77e-11\n 9 河北             [17 × 6] <lm>    497. 0.985 0.368 3.32e-15\n10 河南             [17 × 6] <lm>    663. 0.981 0.375 2.33e-14\n# … with 21 more rows\n\n\n\n使用broom：\n\n\nby_region %>% \n   summarise(tidy(model))\n\n`summarise()` has grouped output by 'Region'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 62 × 6\n# Groups:   Region [31]\n   Region term         estimate  std.error statistic  p.value\n   <chr>  <chr>           <dbl>      <dbl>     <dbl>    <dbl>\n 1 安徽   (Intercept)   942.      89.4        10.5   2.47e- 8\n 2 安徽   gdpPercap       0.327    0.00340    96.2   2.36e-22\n 3 北京   (Intercept) -3824.    1301.         -2.94  1.01e- 2\n 4 北京   gdpPercap       0.392    0.0160     24.4   1.71e-13\n 5 福建   (Intercept)  1502.     214.          7.01  4.21e- 6\n 6 福建   gdpPercap       0.287    0.00471    60.9   2.20e-19\n 7 甘肃   (Intercept)  -168.     319.         -0.528 6.05e- 1\n 8 甘肃   gdpPercap       0.448    0.0182     24.6   1.55e-13\n 9 广东   (Intercept)  -487.     363.         -1.34  1.99e- 1\n10 广东   gdpPercap       0.417    0.00804    51.9   2.40e-18\n# … with 52 more rows\n\nby_region %>% \n   summarise(glance(model))\n\n`summarise()` has grouped output by 'Region'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 31 × 13\n# Groups:   Region [31]\n   Region r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n   <chr>      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\n 1 安徽       0.998         0.998  197.     9260. 2.36e-22     1  -113.  232.\n 2 北京       0.975         0.974 2134.      597. 1.71e-13     1  -153.  313.\n 3 福建       0.996         0.996  441.     3713. 2.20e-19     1  -127.  259.\n 4 甘肃       0.976         0.974  638.      605. 1.55e-13     1  -133.  272.\n 5 广东       0.994         0.994  630.     2696. 2.40e-18     1  -133.  271.\n 6 广西       0.996         0.996  287.     4133. 9.88e-20     1  -119.  245.\n 7 贵州       0.996         0.996  286.     4038. 1.17e-19     1  -119.  244.\n 8 海南       0.955         0.952 1249.      315. 1.77e-11     1  -144.  295.\n 9 河北       0.985         0.985  529.     1019. 3.32e-15     1  -130.  265.\n10 河南       0.981         0.980  706.      783. 2.33e-14     1  -135.  275.\n# … with 21 more rows, and 4 more variables: BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>\n\nby_region %>% \n   summarise(augment(model))\n\n`summarise()` has grouped output by 'Region'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 527 × 9\n# Groups:   Region [31]\n   Region Consumption gdpPercap .fitted  .resid   .hat .sigma .cooksd .std.resid\n   <chr>        <dbl>     <dbl>   <dbl>   <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n 1 安徽          2739     5716.   2811.  -72.5  0.140    203. 1.28e-2    -0.396 \n 2 安徽          2988     6230.   2980.    8.49 0.135    204. 1.67e-4     0.0463\n 3 安徽          3312     6990.   3228.   84.0  0.128    203. 1.53e-2     0.456 \n 4 安徽          3707     8236.   3635.   71.7  0.117    203. 9.91e-3     0.387 \n 5 安徽          3870     9274.   3975. -105.   0.109    202. 1.94e-2    -0.564 \n 6 安徽          4409    10639.   4421.  -12.2  0.0986   204. 2.32e-4    -0.0651\n 7 安徽          5276    12981.   5187.   89.0  0.0842   203. 1.02e-2     0.472 \n 8 安徽          6006    15514.   6015.   -9.30 0.0722   204. 9.32e-5    -0.0490\n 9 安徽          6829    17721.   6737.   92.0  0.0648   202. 8.07e-3     0.482 \n10 安徽          8237    22242.   8216.   21.5  0.0588   204. 3.93e-4     0.112 \n# … with 517 more rows\n\n\n（分组）滚动回归"
  },
  {
    "objectID": "09-descriptive-statistics.html",
    "href": "09-descriptive-statistics.html",
    "title": "10  描述性统计",
    "section": "",
    "text": "描述性统计，主要是通过计算汇总统计量、绘制统计图来描述数据。"
  },
  {
    "objectID": "09-descriptive-statistics.html#一些概念",
    "href": "09-descriptive-statistics.html#一些概念",
    "title": "10  描述性统计",
    "section": "10.1 一些概念",
    "text": "10.1 一些概念\n\n随机变量：一个事件的结果无法预料，这个现象就叫做随机现象。表示随机现象一组结果的变量就是随机变量。\n\n如，调查了 100 个人的身高，这 100 个身高的数据是随机变量身高的 数据。并不是说这些身高值是不固定可变的，而是这100个身高值是一次调 查的结果，再调查100个人就是另一组不同的 100 个身高值。\n\n概率分布：当多个随机结果放在一起时，可以发现一定的规律。随机现象五花八门，但每一种随机现象表现出来的规律性是固定的，用数学语 言表达出来就是概率分布。所以，不同概率分布就是不同随机现象规律性的数学描述。\n\n\n\n\n\n\n\nNote\n\n\n\n同一种概率分布，也不都是相同的，这是由不同参数值决定和区分的。\n统计学最常用到四大概率分布：正态分布、 t 分布、卡方分布、 F 分布。\n\n\n\n概率论与数理统计：概率论就是研究随机现象规律性，即各种概率分布及性质的理论。数理统计所研究的数据是带有随机性的，所以就需要借助概率论中的概率分布理论加以描述和做出统计推断， 所以说：\n\n概率论是数理统计的基础，数理统计是概率论的一种应用。\n\n\n\n\n\n\nNote\n\n\n\nnorm()系列函数：\n\ndnorm(x, mean = 0, sd = 1, log = FALSE)：返回正态分布的概率密度函数值。\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)：返回正态分布的分布函数值。\nqnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)：返回给定概率的p后的后的下分位点。\nrnorm(n, mean = 0, sd = 1)：返回n个正态分布随机数构成的向量。"
  },
  {
    "objectID": "09-descriptive-statistics.html#总体样本参数和统计量",
    "href": "09-descriptive-statistics.html#总体样本参数和统计量",
    "title": "10  描述性统计",
    "section": "10.2 总体、样本、参数和统计量",
    "text": "10.2 总体、样本、参数和统计量\n\n总体，是包含所研究的全部个体（数据）的集合。\n样本，从总体中抽取的一部分个体的集合，样本中包含个体的数目成为样本量。\n参数，用来描述总体特征的概括性值，是研究者想要了解的总体的某些特征值，如总体值（μ\\mu）、总体方差（σ2\\sigma^2）、总体比例（π\\pi）等。\n统计量，是用来描述样本特征的概括性数字度量，是根据样本数据计算出来的量，由于抽样是随机的，因此统计量是样本的函数。与上面总体参数对应的统计量是样本均值（x‾\\bar{x}）、样本标准差（S2S^2）、样本比例（p）等。\n\n由于总体数据通常量大且不确定，故参数通常为未知常数。所以才需进行抽样，根据样本计算出相应的统计量值对总体参数进行估计。\n抽取样本的目的是根据样本数据提供的信息推断总体的特征（或用样本统计量推断总体参数）。"
  },
  {
    "objectID": "09-descriptive-statistics.html#常用的样本统计量",
    "href": "09-descriptive-statistics.html#常用的样本统计量",
    "title": "10  描述性统计",
    "section": "10.3 常用的样本统计量",
    "text": "10.3 常用的样本统计量\n\n中位数：位于数据最中间的数据，比中位数大和小的数据各占观测值的一半。中位数具有稳健性的优点，不受极个别极端数据的影响。\n\n\n\n\n\n\n\nNote\n\n\n\n正态分布的数据用均值描述，偏态分布的数据最好用中位数描述。例如，人均工资给人一种被平均的感觉，中位数工资才是更合适的中间收入。\n\n\n\n分位数（quantile）：\n\n中位数是 0.5 分位数，位于 0.5 位置的数。\n0.25 分位数，称为下四分位数 (Q1), 是位于 0.25 那个位置的数，即比它小的数占比是 0.25, 比它大的数占比是 0.75。\n0.75 分位数，称为上四分位数 (Q3)。\n更一般地，一个有nn个样本的数据集，它的pp分位数，是位于pp位置的数，即比它小的数占比是 pp, 比它大的数占比是 1−p1-p。 或者说npnp的数比它小， n(1−p)n(1-p)的数比它大。\n\n众数（mode）：是观测值中出现次数最多的数，对应了分布的最高峰。众数常用于分类数据，即出现频数最高的值。\n\nrstatix::get_mode(x)：计算向量x的众数。"
  },
  {
    "objectID": "09-descriptive-statistics.html#数据分散程度的统计量",
    "href": "09-descriptive-statistics.html#数据分散程度的统计量",
    "title": "10  描述性统计",
    "section": "10.4 数据分散程度的统计量",
    "text": "10.4 数据分散程度的统计量\n\n极差（range）：数据中最大值和最小值的差。\n四分位距（interquatile range）：上下四分位数之差。\n样本方差：式中n−1n-1即为自由度，是计算样本统计量时能够自由取值的数值的个数。\n\ns2=1n−1∑i−1n|xi−x‾|2\n  s^2 = \\frac{1}{n-1}\\sum^n_{i-1}|x_i-\\bar{x}|^2\n\n\n\n\n\n\n\nNote\n\n\n\n不同统计方法的自由度均不相同，但基本原则是每估计1个参数，就需要消耗一个自由度。\n以回归分析为例，若有m个自变量，则需要估计m+1m+1个参数（包括截距项/常数项），所以模型的F检验用到的自由度是n−(m+1)n-(m+1)。这意味着，只剩下n−(m+1)n-(m+1)个可以自由取值的数值来估计模型误差。\n\n\n\n样本标准差（sd）：样本的方差的平方根即为标准差s，量纲与原数据一致。\n变异系数(coefficient of variation)：标准差占均值的百分比，可用于比较不同量纲数据的分散性。\n\ncv=sx‾(%)\n  c_v=\\frac{s}{\\bar{x}}(\\%)\n\nR中的实现：\n\nmax(x) - min(x)  # 计算数值向量x的极差\nIQR(x)  # 计算数值向量x的四分位距\nvar(x)  # 计算数值向量x的样本方差\nsd(x)  # 计算数值向量x的样本标准差"
  },
  {
    "objectID": "09-descriptive-statistics.html#数据分布形状的统计量",
    "href": "09-descriptive-statistics.html#数据分布形状的统计量",
    "title": "10  描述性统计",
    "section": "10.5 数据分布形状的统计量",
    "text": "10.5 数据分布形状的统计量\n\n偏度（skewness），刻画数据是否对称的指标，其中，均值对称的数据偏度为0（不偏）；右拖尾的数据偏度为正（右偏）；左拖尾的数据偏度为负（左偏）。\n\nSK=n(n−1)(n−2)∑i=1n(xi−x‾s)3\n  SK=\\frac{n}{(n-1)(n-2)}\\sum^n_{i=1}(\\frac{x_i-\\bar{x}}{s})^3\n\n\n丰度（kurtosis）：刻画数据是否尖峰的指标\n\nK=n(n+1)(n−1)(n−2)(n−3)∑n−1n(xi−x‾s)4−3(n−12)(n−2)(n−1)\nK=\\frac{n(n+1)}{(n-1)(n-2)(n-3)}\\sum^n_{n-1}(\\frac{x_i-\\bar{x}}{s})^4-\\frac{3(n-1^2)}{(n-2)(n-1)}\n\n峰度是以标准正态分布为基准，标准正态分布的峰度为 0; 尖峰薄尾的分布峰度为正；平峰厚尾的分布峰度为负\ndatawizard包提供了skewness()和kurtosis()函数，分别计算偏度和丰度。\n\n\n\n\n\n\nNote\n\n\n\n很多包都提供了同时对多个变量（分组）描述汇总所有常见统计量的函数，其中tidy风格的是rstatix::get_summary_stats()和dlookr::describe()。\n\nlibrary(rstatix)\nlibrary(dlookr)\niris %>% \n  group_by(Species) %>% \n  get_summary_stats(type = \"full\")\n\n# A tibble: 12 × 14\n   Species variable     n   min   max median    q1    q3   iqr   mad  mean    sd\n   <fct>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 setosa  Petal.L…    50   1     1.9   1.5   1.4   1.58 0.175 0.148 1.46  0.174\n 2 setosa  Petal.W…    50   0.1   0.6   0.2   0.2   0.3  0.1   0     0.246 0.105\n 3 setosa  Sepal.L…    50   4.3   5.8   5     4.8   5.2  0.4   0.297 5.01  0.352\n 4 setosa  Sepal.W…    50   2.3   4.4   3.4   3.2   3.68 0.475 0.371 3.43  0.379\n 5 versic… Petal.L…    50   3     5.1   4.35  4     4.6  0.6   0.519 4.26  0.47 \n 6 versic… Petal.W…    50   1     1.8   1.3   1.2   1.5  0.3   0.222 1.33  0.198\n 7 versic… Sepal.L…    50   4.9   7     5.9   5.6   6.3  0.7   0.519 5.94  0.516\n 8 versic… Sepal.W…    50   2     3.4   2.8   2.52  3    0.475 0.297 2.77  0.314\n 9 virgin… Petal.L…    50   4.5   6.9   5.55  5.1   5.88 0.775 0.667 5.55  0.552\n10 virgin… Petal.W…    50   1.4   2.5   2     1.8   2.3  0.5   0.297 2.03  0.275\n11 virgin… Sepal.L…    50   4.9   7.9   6.5   6.22  6.9  0.675 0.593 6.59  0.636\n12 virgin… Sepal.W…    50   2.2   3.8   3     2.8   3.18 0.375 0.297 2.97  0.322\n# … with 2 more variables: se <dbl>, ci <dbl>\n\niris %>% \n  group_by(Species) %>% \n  dlookr::describe()\n\n# A tibble: 12 × 27\n   described_variables Species        n    na  mean    sd se_mean   IQR skewness\n   <chr>               <fct>      <int> <int> <dbl> <dbl>   <dbl> <dbl>    <dbl>\n 1 Petal.Length        setosa        50     0 1.46  0.174  0.0246 0.175   0.106 \n 2 Petal.Length        versicolor    50     0 4.26  0.470  0.0665 0.600  -0.607 \n 3 Petal.Length        virginica     50     0 5.55  0.552  0.0780 0.775   0.549 \n 4 Petal.Width         setosa        50     0 0.246 0.105  0.0149 0.1     1.25  \n 5 Petal.Width         versicolor    50     0 1.33  0.198  0.0280 0.3    -0.0312\n 6 Petal.Width         virginica     50     0 2.03  0.275  0.0388 0.5    -0.129 \n 7 Sepal.Length        setosa        50     0 5.01  0.352  0.0498 0.400   0.120 \n 8 Sepal.Length        versicolor    50     0 5.94  0.516  0.0730 0.7     0.105 \n 9 Sepal.Length        virginica     50     0 6.59  0.636  0.0899 0.675   0.118 \n10 Sepal.Width         setosa        50     0 3.43  0.379  0.0536 0.475   0.0412\n11 Sepal.Width         versicolor    50     0 2.77  0.314  0.0444 0.475  -0.363 \n12 Sepal.Width         virginica     50     0 2.97  0.322  0.0456 0.375   0.366 \n# … with 18 more variables: kurtosis <dbl>, p00 <dbl>, p01 <dbl>, p05 <dbl>,\n#   p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>,\n#   p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>,\n#   p99 <dbl>, p100 <dbl>"
  },
  {
    "objectID": "09-descriptive-statistics.html#统计图",
    "href": "09-descriptive-statistics.html#统计图",
    "title": "10  描述性统计",
    "section": "10.6 统计图",
    "text": "10.6 统计图\n\n10.6.1 分类数据的统计图\n\n条形图：\n\n\ngeom_bar()：对原始数据绘制\ngeom_col()：对汇总频数/频率的数据绘制\n\n\nlibrary(tidyverse)\ndf <- starwars %>% \n  # 合并频数<=5的类别\n  mutate(skin_color = fct_lump(skin_color, n = 5)) %>% \n  count(skin_color, sort = TRUE) %>% \n  mutate(p = n/sum(n))\n\nggplot(df, aes(x = fct_reorder(skin_color, p),\n               y = p)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"皮肤颜色\", y = \"占比\") +\n  geom_text(aes(y = p + 0.04,\n                label = str_c(round(p*100, 1), \"%\")), \n            color = \"red\",\n            size = 5)\n\n\n\n\n\n\n10.6.2 连续数据的统计图\n\n10.6.2.1 直方图\n连续数据利用直方图估计总体的概率密度。\n\n直方图是用面积而不是高度来表示数据。将变量的取值范围分成若干区间，区间总面积为100%，每个矩形的面积为落在该区间内数据的频率：\n\n矩形的高=频率区间长度=密度\n  矩形的高=\\frac{频率}{区间长度}=密度\n\n\n\n\n\n\n\nNote\n\n\n\n特别的，如果区间是等长的，则矩形的高是就是频率。\n\n\n\n使用geom_histogram()绘制直方图。频率直方图与概率密度曲线正好搭配，因为频率释放图的条形宽趋近于0，就是概率密度曲线。\n如果想绘制直方图 + 概率密度曲线，则需要对密度进行放大：条形宽度*样本数条形宽度*样本数倍。\n\n\nset.seed(123)\ndf <- tibble(heights = rnorm(10000, \n                             mean = 170,\n                             sd = 2.5))\nggplot(df, aes(x = heights)) +\n  geom_histogram(fill = \"steelblue\",\n                 color = \"black\",\n                 binwidth = 0.5) +\n  stat_function(\n    fun = ~ dnorm(.x, mean = 170, sd = 2.5) * 0.5 * 10000,\n    color = \"red\"\n  )\n\n\n\n\n\n\n\n\n\n\nDanger\n\n\n\n\n如果想在同一张图上叠加多个直方图，一对比分类变量不同水平的概率分布，更适合使用geom_freqploy()绘制频率多边形图。\ngeom_density()绘制核密度估计曲线。\n\n\n\n\n\n10.6.2.2 箱线图\n\n以数据的上下四分位数为界画一个矩形盒子（数据中处于中间的50%的数据落在盒内）。\n在数据的中位数位置画一条线段作为中位线。\n默认延长线为盒长的1.5倍，之外的点为异常值。\n\n箱线图的的主要应用为：剔除数据异常值、判断数据的偏态和重尾、可视化组间差异。\n\nggplot(mpg, aes(drv, hwy)) +\n  geom_boxplot()"
  },
  {
    "objectID": "09-descriptive-statistics.html#列联表",
    "href": "09-descriptive-statistics.html#列联表",
    "title": "10  描述性统计",
    "section": "10.7 列联表",
    "text": "10.7 列联表\n对分类变量做描述统计，通常是计算各水平值出现的频数和占比，得到列联表（交叉表）。传统方法可以使用table()函数实现，而janitor包中提供了更加tidy的方法。\n\n\n\n\n\n\nNote\n\n\n\n\n列联表（contingency table）是观测数据按两个或更多属性（定性变量）分类时所列出的频数表。它是由两个以上的变量进行交叉分类的频数分布表。\n列联表又称交互分类表，所谓交互分类，是指同时依据两个变量的值，将所研究的个案分类。交互分类的目的是将两变量分组，然后比较各组的分布状况，以寻找变量间的关系。\n\n\n\njantior::tabyl()函数可以生成一个、两个、三个变量的列联表，在结合adorn_*()系列函数，可以很方便的按照想要的格式添加行列合计、占比等。\n\nlibrary(janitor)\nmpg %>% \n  tabyl(drv) %>% \n  adorn_totals(\"row\") %>%  # 添加合计行\n  adorn_pct_formatting()  # 设置百分比格式\n\n   drv   n percent\n     4 103   44.0%\n     f 106   45.3%\n     r  25   10.7%\n Total 234  100.0%\n\n\n二维列联表，添加列占比和频数。\n\nmpg %>% \n  tabyl(drv, cyl) %>% \n  adorn_percentages(\"col\") %>%   # 添加列占比\n  adorn_pct_formatting() %>%  # 设置百分比格式\n  adorn_ns()  # 添加频数\n\n drv          4          5          6          8\n   4 28.4% (23)   0.0% (0) 40.5% (32) 68.6% (48)\n   f 71.6% (58) 100.0% (4) 54.4% (43)  1.4%  (1)\n   r  0.0%  (0)   0.0% (0)  5.1%  (4) 30.0% (21)\n\n\n此外，还有很多包能将描述性统计回归模型的结果变成规范的表格样式，代表性的是gtsummary包；表格设计、实验设计在科研、生产中的应用非常广泛，各种常用的实验设计，可以使用DoE.base包实现。"
  },
  {
    "objectID": "10-parameter-estimation.html",
    "href": "10-parameter-estimation.html",
    "title": "11  参数估计",
    "section": "",
    "text": "Note\n\n\n\n\n与总体有关的指标参数；与样本有关的指标是统计量。\n统计推断的重要内容之一是参数估计，即在抽样及抽样分布的基础上，根据样本统计量来推断所关心的总体参数。"
  },
  {
    "objectID": "10-parameter-estimation.html#点估计与区间估计",
    "href": "10-parameter-estimation.html#点估计与区间估计",
    "title": "11  参数估计",
    "section": "11.1 点估计与区间估计",
    "text": "11.1 点估计与区间估计\n参数估计主要有两种：\n\n点估计（准确，但不一定可靠）：就是样本统计量。比如估计哈尔滨成年男性的平均身高，样本均值175cm就是点估计；有一定把握落在172-178cm之间，就是区间估计。\n区间估计（可靠，但不很精确）：通常是指估计其95%置信区间，即有95%的把握认为该区间包含了总体参数，换言之，如果抽样100次，将有95次该区间包含了总体参数。\n\n置信区间越窄反映了参数估计的精度越高，影响它的因素有两个：置信区间：置信区间是指由样本统计量所构造的总体参数的估计区间。在统计学中，一个概率样本的置信区间（Confidence interval）是对这个样本的某个总体参数的区间估计。置信区间展现的是这个参数的真实值有一定概率落在测量结果的周围的程度，其给出的是被测量参数的测量值的可信程度\n\n置信水平，置信水平越高置信宽度越大。\n样本量，样本量越大，置信宽度越小。\n\n\n11.1.1 使用标准误计算置信区间\n即使一个代表性非常好的样本，也无法完全与总体等同，总会存在一定的抽样误差。\n\n\n\n\n\n\nTip\n\n\n\n\n例如用100人的平均身高作为总体参数μ\\mu的估计，如果在随机抽样100人，又得到另一个平均身高……依次类推做10次抽样，就可以计算出样本统计量：10个平均身高和10个标准差。这10个平均身高也可以计算标准差（即标准误，样本统计量的标准差），它反映了样本统计量之间差别（即抽样误差）的大小。\n而实际中不可能多次抽样计算每个样本的统计量，再计算各统计量间的差异。通长我们获取一个尽可能大的样本来计算标准误，借助以下公式。式 Equation 11.1 中，s为样本标准差，n为样本量。\n\n计算具体标准误时，真正需要的可能是某些真实值或来自总体的值。若无法得到，则通常使用它们所对应的样本估计值来代替，某些估计值要保证能作为代替，可能离不开一些模型假定。se=sn(11.1)\n    se = s\\sqrt{n}\n \\qquad(11.1)\n\n\n标误几乎在所有的统计方法中都会出现，因为其大小直接发硬了抽样是否有足够的代表性，进而考察得出的结果是否有可靠性。\n由于抽样误差的存在，如果用样本统计量直接估计总体参数，则肯定会有一定偏差。所以在估计总体参数时，需要考虑到这种偏差大小，即用置信区间（参数估计值±估计误差参数估计值 \\pm 估计误差）来估计总体参数。\n根据中心极限定理，从任何分布中抽样，只要样本量足够大，其统计量最终会服从正态分布。因此，估计误差通常用对应一定正态分位数的z值在乘以抽样误差的标准误来表示。例如，95%置信区间一般表示为参数估计值±1.96×标准误参数估计值 \\pm 1.96 \\times 标准误中心极限定理：在自然界与生产中，一些现象受到许多相互独立的随机因素的影响，如果每个因素所产生的影响都很微小时，总的影响可以看作是服从正态分布的。中心极限定理就是从数学上证明了这一现象。\n\n\n11.1.2 Bootstrap法估计置信区间\n对于某些抽样分布未知或难以计算的统计量，就需要使用Bootstrap（自助）重抽样法1来研究抽样样本变化所带来的变异。\nBootstrap 法的基本思想是：样本是从总体中随机抽取的，则包含总体的全部信息，那么不妨就把该样本视为” 总体”，进行多次有放回抽样生成一系列经验样本，再对每个经验样本计算统计量，就可以得到统计量的分布，进而用于统计推断。\n\n\ntidymodels 系列的 infer 包提供了统一的、tidy 的统计推断工作流，主要函数有：\n\nspecify(): 设定感兴趣的变量或变量关系\nhypothesize(): 设定零假设\ngenerate(): 基于零假设生成数据\ncalculate(): 根据上述数据，计算统计量的分布\nvisualize(): 可视化\n\n还有获取/绘制 p 值/置信区间的函数。\nBootstrap法估计统计量置信区间的基本步骤如下：\n\n从原始样本中有会放的随机抽取n个个体构成子样本。\n对子样本计算想要的统计量。\n重复前两步K次，得到K个统计量的估计值。\n根据K个估计值获得统计量的分布，并计算置信区间。\n\n\n\n11.1.3 案例\n假设某学校随机抽样了20名学生身高，想要估计该学校所有学生的平均身高。更多案例，可参阅infer包Vignettes。\n\n计算基于标准误的置信区间\n\n\nlibrary(tidyverse)\ndf <- tibble(\n    height = c(167,155,166,161,168,163,179,164,178,156,\n               161,163,168,163,163,169,162,174,172,172))\nmu <- mean(df$height)  # 点估计：计算样本均值\nmu\n\n[1] 166.2\n\nse <- sd(df$height) / sqrt(nrow(df))  # 计算标准误\nc(mu - qnorm(1 - 0.05/2) * se, mu + qnorm(1 - 0.05/2) * se)\n\n[1] 163.3683 169.0317\n\n\n\n计算基于Bootstrap法的置信区间\n\n\nlibrary(infer)\nboot_means <- df %>% \n    specify(response = height) %>% \n    generate(reps = 1000, type = \"bootstrap\") %>%  # 1000次bootstrap\n    calculate(stat = \"mean\")  # 计算统计量，样本均值\nboot_means\n\nResponse: height (numeric)\n# A tibble: 1,000 × 2\n   replicate  stat\n       <int> <dbl>\n 1         1  167.\n 2         2  167.\n 3         3  165.\n 4         4  163.\n 5         5  167.\n 6         6  166 \n 7         7  166.\n 8         8  167.\n 9         9  168.\n10        10  165.\n# … with 990 more rows\n\n# 计算置信区间\nboot_ci <- boot_means %>% \n    get_ci(level = 0.95, type = \"percentile\")\nboot_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1     163.     169.\n\n# 可视化\nvisualise(boot_means) +\n    shade_ci(endpoints = \n    boot_ci)"
  },
  {
    "objectID": "10-parameter-estimation.html#最小二乘估计ols",
    "href": "10-parameter-estimation.html#最小二乘估计ols",
    "title": "11  参数估计",
    "section": "11.2 最小二乘估计（OLS）",
    "text": "11.2 最小二乘估计（OLS）\nOrdinary Least Squares，最小二乘估计，常用于估计线性回归、曲线拟合的参数。其思想是让实际值与模型之的总偏离达到最小，从而得到最优的模型参数估计值。\n用一元线性回归来阐述，如有10组广告费用与销售的数据。\n\nsales <- tibble(\n    cost = c(30,40,40,50,60,70,70,70,80,90),\n    sale = c(143.5,192.2,204.7,266,318.2,457,333.8,312.1,\n             386.4,503.9))\n\nggplot(sales, aes(cost, sale)) +\n    geom_point()\n\n\n\n\n图中的散点大致在一条直线上，而一元线性回归就是寻找一条直线，使得这条直线与散掉拟合程度最好（即越接近直线越好）\n\nm <- lm(sale ~ cost, sales)\nsales1 <- sales[c(6, 9), ] %>% \n  mutate(p = predict(m, .))\n\nggplot(sales, aes(cost, sale)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_segment(aes(x = cost, y = sale,\n    xend = cost, yend = p), \n    data = sales1, linetype = 2,\n    color = \"red\")\n\n\n\n\nR中，线性回归的最小二乘估计可用lm()实现，非线性回归的最小二乘估计可用nls()函数实现。\n\n11.2.1 非线性拟合\n现有我国2003-2019年历年累积电影票房数据\n\ndf <- readxl::read_xlsx(\"datas/历年累计票房.xlsx\") %>% \n  mutate(year = year - 2002)\ndf\n\n# A tibble: 17 × 2\n    year boxoffice\n   <dbl>     <dbl>\n 1     1       8  \n 2     2       9.2\n 3     3      15.1\n 4     4      20.5\n 5     5      33.3\n 6     6      43.4\n 7     7      62.1\n 8     8     102. \n 9     9     131. \n10    10     171. \n11    11     218. \n12    12     296. \n13    13     441. \n14    14     457. \n15    15     559. \n16    16     610. \n17    17     643. \n\np <- ggplot(df, aes(year, boxoffice)) +\n  geom_point(color = \"red\", size = 1.5)\np\n\n\n\n\n非线性回归的第一步是找到合适的模型函数。这些三点大致服从Logistic分布曲线：$N(t)=\n我们想用nls()做非线性拟合，即寻找上述公式中的参数ψ1,ψ2,ψ3\\psi_1, \\psi2, \\psi3的最优解。非线性拟合的算法非常依赖于参数初始值的选取，选取适当（离估计值不远），就能很快收敛到最优估计，否则迭代可能无法收敛。\n参数ψ1\\psi_1对应人口容纳量上线，大致为曲线拐点值（目测约为400）的2倍。一旦确定ψ1\\psi_1，则可以确定剩余两个参数：\nlogit(N(t)ψ1)=ψ2+ψ3t\n  logit(\\frac{N(t)}{\\psi_1})=\\psi_2+\\psi_3t\n\n其中，logit(p)=lnp1−plogit(p)=ln\\frac{p}{1-p}称为Logit变换。\n接下来使用lm()做线性回归得到ψ2,ψ3\\psi_2,\\psi_3的估计值。\n\nlm.fit <- lm(car::logit(boxoffice / 800) ~ year, df)\ncoef(lm.fit)\n\n(Intercept)        year \n -5.1446518   0.3913851 \n\n\n如此一来，我们得到一组较好的参数初始值：\nψ1=800,ψ2=−5.14,ψ3=0.39\\psi_1=800,\\psi_2=-5.14,\\psi_3=0.39\n使用nls()函数做非线性拟合。需要提供模型公式和初始参数值。\n\nlog.fit <- nls(boxoffice ~ phi1 / (1+exp(-(phi2   \n+phi3*year))),\n  data = df,\n  start = list(phi1=800, phi2=-5.14, phi3=0.39))\ncoefs = coef(log.fit)\ncoefs\n\n       phi1        phi2        phi3 \n760.5771289  -5.4570682   0.4265685 \n\n# 绘图看拟合效果\nLogFit  <-  function(x) coefs[1] / (1+exp(-(coefs[2]+coefs[3]*x)))\n\np + \n  geom_function(fun=LogFit, color=\"steelblue\", size=1.2)\n\n\n\n\n\n\nnls()拟合结果依赖于初始值和selfstart设置，容易拟合失败。可以进一步使用glsnls包。"
  },
  {
    "objectID": "10-parameter-estimation.html#sec:MLE",
    "href": "10-parameter-estimation.html#sec:MLE",
    "title": "11  参数估计",
    "section": "11.3 最大似然估计(MLE)",
    "text": "11.3 最大似然估计(MLE)\n\n\n\n\n\n\nWarning\n\n\n\n进一步学习补充"
  },
  {
    "objectID": "11-Hypothesis-testing.html",
    "href": "11-Hypothesis-testing.html",
    "title": "12  假设检验",
    "section": "",
    "text": "假设检验的P值，是在H0H_0为真时根据检验统计量服从的理论概率分布计算的，衡量的是在原假设H0H_0下出现当前观测结果可能性的大小。↩︎"
  },
  {
    "objectID": "12-regression.html",
    "href": "12-regression.html",
    "title": "13  回归分析",
    "section": "",
    "text": "回归分析是统计学核心算法，是计量模型和机器学习的最基本算法。计量模型和机器学习中的各种回归算法都可以看作是线性回归的扩展，即使是分类算法也可以看做是一种特殊的回归\n回归分析是确定两个或两个以上变量相互依赖的定量关系的一种统计分析方法。具体是通过多组自变量和因变量的样本数据，拟合出最佳的函数关系。\n回归分析通常用于："
  },
  {
    "objectID": "12-regression.html#回归建模的数学描述",
    "href": "12-regression.html#回归建模的数学描述",
    "title": "13  回归分析",
    "section": "13.1 回归建模的数学描述",
    "text": "13.1 回归建模的数学描述\n设y为因变量数据，x为自变量数据，假设两者间的真实（精确）关系为：\ny=f(x)\ny=f(x)\n\n但在实际情况下，回归模型只能试图找到一种近似的关系代替它：\nf̂(x)≈f(x)\n\\hat{f}(x) \\approx f(x)\n\n两者之差就是模型的残差：\nξ=f(x)−f̂(x)\n\\xi = f(x)-\\hat{f}(x)\n\n我们总是希望把y和x的关系都留在模型部分，让残差部分最好只是白噪声（完全是随机误差，0均值，微小标准差的正态分布）：\nξN(0,θξ2)\n\\xi {~} N(0,\\theta_\\xi^2)\n\n此时说明回归模型建模成功，否则，就是模型尚未提取出充分的模型关系（欠拟合）。\n\n\n\n\n\n\nNote\n\n\n\n回归模型的基本原则是：在没有显著差异的情况下，优先选择更简单的模型。如果更简单的模型已经足已充分建模，强行使用复杂模型会产生过拟合，反而降低模型的泛化（预测）能力"
  },
  {
    "objectID": "22-appendix.html",
    "href": "22-appendix.html",
    "title": "Appendix B — R Markdown",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:"
  },
  {
    "objectID": "22-appendix.html#including-plots",
    "href": "22-appendix.html#including-plots",
    "title": "Appendix B — R Markdown",
    "section": "B.1 Including Plots",
    "text": "B.1 Including Plots\nYou can also embed plots, for example:\n\npar(mar = c(4, 4, .1, .1))\nplot(pressure)\n\n\n\n\nHere is another nice figure!\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "12-regression.html#回归诊断",
    "href": "12-regression.html#回归诊断",
    "title": "13  回归分析",
    "section": "13.2 回归诊断",
    "text": "13.2 回归诊断\n\n13.2.1 拟合优度检验\n即 R2R^2，也成为可决系数（决定系数），反映了自变量能解释的方差占总方差的百分比。R2R^2 值越大，说明模型拟合的结果越好。R2R^2 的计算未考虑自由度的问题，为避免增加自变量而高估 R2R^2，对 R2R^2 进行调整是更合理的：Radj2=1−n−1n−p−1×(1−R2)R^2_{adj}=1-\\frac{n-1}{n-p-1} \\times (1-R^2), n为样本数，p为自变量个数。\n\n\n13.2.2 均方误差和均方根误差\n均方误差：\nMSE=1n∑i=1n(yi−yî)2\nMSE = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y_i})^2\n\n均方根误差：刻画预测值与真实值平均偏离情况，是所有回归模型（包括机器学习中的回归算法）中最常用的性能评估指标。\nRMSE=1n∑i=1n(yi−yî)2\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y_i})^2}\n\n\n\n13.2.3 残差检验\n\nknitr::include_graphics(\"images/residual.jpg\", dpi = FALSE)\n\n\n\n\nFigure 13.1: 残差分类图\n\n\n\n\n\n只有图 Figure 13.1 （a）可以说明模型拟合成功，将模型的所有部分有均提取出来。\n（e）和（f）说明模型有问题，没有把模型部分提取完全。\n（b）说明数据有异常点，应剔除后重新建模。\n（c）的残差随x的增加而增加，（d）残差随x显增加后减小，都属于异方差。\n\n\n13.2.3.1 残差正态性检验、残差独立性、检验异方差检验\n\n\n参看第22讲 回归分析部分\n\n\n\n13.2.4 共线性回归\n多元线性回归建模，若自变量数据之间存在较强的线性相关性，即存在多重共线性。此现象会导致回归模型不稳定，得到的模型被称为伪回归模型，并不能反映自变量与因变量之间的真实关系。\n多元线性回归建模，需要做共线性诊断，识别出多重共线性，并处理多重共线性再建模。\n多重共线性的解决办法（任选其一）：\n\n若两个自变量线性相关系数较大，则只用其中一个。\n使用逐步回归，剔除冗余自变量。\n使用主成分回归，相当于对自变量进行重组，在做回归。\n使用正则化回归：岭回归、Lasso回归、弹性网模型（前两组合）。\n\n\n\n13.2.5 回归系数的检验\n\n13.2.5.1 回归系数的显著性检验\n\n\n13.2.5.2 回归标准误与回归系数标准误\n\n\n\n13.2.6 回归模型预测"
  },
  {
    "objectID": "22-R-workflow.html",
    "href": "22-R-workflow.html",
    "title": "Appendix A — R Markdown",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:"
  },
  {
    "objectID": "22-R-workflow.html#including-plots",
    "href": "22-R-workflow.html#including-plots",
    "title": "Appendix A — R Markdown",
    "section": "A.1 Including Plots",
    "text": "A.1 Including Plots\nYou can also embed plots, for example:\n\npar(mar = c(4, 4, .1, .1))\nplot(pressure)\n\n\n\n\nHere is another nice figure!\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "12-regression.html#多元线性回归实例",
    "href": "12-regression.html#多元线性回归实例",
    "title": "13  回归分析",
    "section": "13.3 多元线性回归实例",
    "text": "13.3 多元线性回归实例\n\n13.3.1 准备数据与简单探索\n\n# 企鹅的数据集 penguins，包含 333 个样本，是有关企鹅的特征信息，包括种类、岛屿、嘴长、嘴宽、鳍长、性别。\n# 探索企鹅的体重与这些特征间的关系\nlibrary(tidyverse)\npenguins <- read_csv(\"datas/penguins.csv\")  %>% \n  mutate(species = factor(species))\npenguins\n\n# A tibble: 333 × 7\n   species island    bill_length bill_depth flipper_length body_mass sex   \n   <fct>   <chr>           <dbl>      <dbl>          <dbl>     <dbl> <chr> \n 1 Adelie  Torgersen        39.1       18.7            181      3750 male  \n 2 Adelie  Torgersen        39.5       17.4            186      3800 female\n 3 Adelie  Torgersen        40.3       18              195      3250 female\n 4 Adelie  Torgersen        36.7       19.3            193      3450 female\n 5 Adelie  Torgersen        39.3       20.6            190      3650 male  \n 6 Adelie  Torgersen        38.9       17.8            181      3625 female\n 7 Adelie  Torgersen        39.2       19.6            195      4675 male  \n 8 Adelie  Torgersen        41.1       17.6            182      3200 female\n 9 Adelie  Torgersen        38.6       21.2            191      3800 male  \n10 Adelie  Torgersen        34.6       21.1            198      4400 male  \n# … with 323 more rows\n\n\n\n探索因变量body_mass(体重)的分布：\n\n如果因变量是右偏分布，可以尝试做对数变换变成近似正态分布，此处不做变换\nggplot(penguins, aes(body_mass)) +\n  geom_histogram(bin = 20, \n  fill = \"steelblue\",\n  color = \"black\")\n\n\n\n\n\n\n13.3.2 构建多元线性回归模型\nlm(formula, data, …)：拟合多元线性回归模型。\n\nformula设定模型公式，遵从Wilkinson表示规则，这里列举一些常用的写法：\n\ny ~ .：包含所有自变量。\nx1 : x2：交互效应，即 x1x2x_1x_2 项。\nx1 * x2：包括主效应和交互效应，即 x1+x2+x1x2x_1+x_2+x_1x_2 的简写。\nI()：打包式子作为整体。\ny ~ ploy(x, 2, raw = TRUE)： 一元二次多项式回归，同 y ~ x + I(x^2)。\ny ~ ploym(x1, x2, degree = 2, raw = TRUE)： 二元二次多项式回归。\nlog(y) ~ x： 对y做对数变换。\n\n\n这里我们先将所有自变量都用上，构建初始的多元线性回归模型（往往不是成功的模型）\n\nmdl0 <- lm(body_mass ~ ., penguins)\n\n\n\n13.3.3 共线性针对与逐步回归\n\n使用car::vif()诊断回归模型的多重共线性：\n\nmctest::imcdiag() 诊断回归模型的多重共线性更全面，除了计算 VIF 值外，还计算其他诊断指标值。\ncar::vif(mdl0)\n\n                    GVIF Df GVIF^(1/(2*Df))\nspecies        63.523199  2        2.823144\nisland          3.731695  2        1.389878\nbill_length     6.099673  1        2.469752\nbill_depth      6.101621  1        2.470146\nflipper_length  6.797579  1        2.607217\nsex             2.326898  1        1.525417\n\n\n只有分类变量的species的VIF值较大，其余均小于10，说明不存在共线性。\n\n使用step()函数做逐步回归，剔除不显著的和共线性的自变量。逐步回归是以 AIC 值（越小越好）作为加入和剔除变量的判别条件，参数direction 设置逐步选择的方法： “both”, “backward”（逐步剔除）,“forward”（逐步加入）。\n\nAkaike 信息准则（AIC）常用来比较不同回归模型的拟合效果，优点是既考虑模型的拟合效果又对模型参数过多施加一定惩罚。\nmdl1 <- step(mdl0, direction = \"backward\", trace = 0)\nsummary(mdl1)\n\n\nCall:\nlm(formula = body_mass ~ species + bill_length + bill_depth + \n    flipper_length + sex, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-779.65 -173.18   -9.05  186.61  914.11 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      -1460.995    571.308  -2.557 0.011002 *  \nspeciesChinstrap  -251.477     81.079  -3.102 0.002093 ** \nspeciesGentoo     1014.627    129.561   7.831 6.85e-14 ***\nbill_length         18.204      7.106   2.562 0.010864 *  \nbill_depth          67.218     19.742   3.405 0.000745 ***\nflipper_length      15.950      2.910   5.482 8.44e-08 ***\nsexmale            389.892     47.848   8.148 7.97e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 287.3 on 326 degrees of freedom\nMultiple R-squared:  0.875, Adjusted R-squared:  0.8727 \nF-statistic: 380.2 on 6 and 326 DF,  p-value: < 2.2e-16\n\n\n结果给出了回归系数的标准误、显著性、回归模型的标准误等信息，基于理论的回归系数的置信区间，可以使用confint()提取。\n\nconfint(mdl1)\n\n                        2.5 %     97.5 %\n(Intercept)      -2584.910585 -337.07867\nspeciesChinstrap  -410.980430  -91.97296\nspeciesGentoo      759.746330 1269.50699\nbill_length          4.224516   32.18434\nbill_depth          28.380131  106.05513\nflipper_length      10.226261   21.67423\nsexmale            295.761031  484.02202\n\n\n由结果看出，该模型基本拟合成功，回归系数是显著的，模型的调整 R2R_2 为0.873。\n\n# 计算模型均方根误差\nlibrary(modelr)\nrmse(mdl1, penguins)\n\n[1] 284.3022"
  }
]